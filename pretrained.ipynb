{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import cv2\n",
    "from plotting import imshow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, filenames, images_data, transform=None):\n",
    "        self.filenames = filenames # image name\n",
    "        self.images_data = images_data # image data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Creating image from the dataset\n",
    "        img_data = self.images_data[idx]\n",
    "        img = np.array(img_data, dtype=np.uint8)\n",
    "\n",
    "        R_channel = img[0:1024].reshape(32, 32, 1)\n",
    "        G_channel = img[1024:2048].reshape(32, 32, 1)\n",
    "        B_channel = img[2048:].reshape(32, 32, 1)\n",
    "\n",
    "        image = np.concatenate([R_channel,G_channel,B_channel], axis = 2)\n",
    "    \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, self.filenames[idx]\n",
    "\n",
    "# Normalization and transforming to a tensor\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "train_set = unpickle('train')\n",
    "\n",
    "images_data = train_set[b'data']\n",
    "\n",
    "filenames = [f.decode('utf-8') for f in train_set[b'filenames']]\n",
    "\n",
    "dataset = CustomDataset(filenames=filenames, images_data=images_data, transform=transform)\n",
    "train_loader = DataLoader(dataset, batch_size=8, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    image_tensor, filename = batch  # Extract data from the batch\n",
    "    for idx, image_rgb in enumerate(image_tensor):\n",
    "        resized_img = F.interpolate(image_rgb.unsqueeze(0), size=(224, 224), mode=\"bilinear\", align_corners=False).squeeze(0)\n",
    "\n",
    "        # Convert to NumPy for visualization\n",
    "        resized_img_numpy = resized_img.permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "        # Plot the original and resized images\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "        ax[0].imshow(image_rgb.permute(1, 2, 0).cpu().numpy())\n",
    "        ax[0].set_title(\"Original 32x32 Image\")\n",
    "        ax[0].axis(\"off\")\n",
    "\n",
    "        ax[1].imshow(resized_img_numpy)\n",
    "        ax[1].set_title(\"Resized 224x224 Image\")\n",
    "        ax[1].axis(\"off\")\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ColorizationUNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ColorizationUNet, self).__init__()\n",
    "        \n",
    "        # Use a pretrained MobileNetV2 as an encoder\n",
    "        mobilenet = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.DEFAULT)\n",
    "        self.encoder = mobilenet.features  # Extract feature layers\n",
    "        \n",
    "        # Decoder layers to upscale to 32x32\n",
    "        self.upconv1 = nn.ConvTranspose2d(1280, 256, kernel_size=4, stride=2, padding=1)  # 8x8 -> 16x16\n",
    "        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1)  # 16x16 -> 32x32\n",
    "        self.final_conv = nn.Conv2d(128, 2, kernel_size=3, padding=1)  # Output AB channels (Lab space)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = F.relu(self.upconv1(x))\n",
    "        x = F.relu(self.upconv2(x))\n",
    "        x = torch.tanh(self.final_conv(x))  # Output in range [-1,1] for AB channels\n",
    "        return x\n",
    "\n",
    "# Instantiate model\n",
    "model = ColorizationUNet()\n",
    "\n",
    "# Test on a dummy grayscale image (batch_size=1, channels=3, height=32, width=32)\n",
    "dummy_input = torch.randn(1, 3, 32, 32)  # MobileNet expects 3-channel input\n",
    "output = model(dummy_input)\n",
    "print(\"Output shape:\", output.shape)  # Should be (1, 2, 32, 32)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
