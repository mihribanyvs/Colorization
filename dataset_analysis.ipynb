{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "from plotting import imshow\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnalysisDataset(Dataset):\n",
    "    def __init__(self, filenames, images_data, bin_count=50):\n",
    "        self.filenames = filenames # image name\n",
    "        self.images_data = images_data # images data: data used to construct the images \n",
    "        self.bin_count = bin_count # number of bins used \n",
    "        self.bins = np.linspace(0, 1, bin_count)  # Bin edges\n",
    "        # we are creating bins between 0 and 1 in order to have the values of u and v\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "    \n",
    "    def bin_labels(self, values):\n",
    "        return np.digitize(values, self.bins) -1 # Map values to bin indices\n",
    "                                                   # This function takes values and assigns each value to a bin index\n",
    "                                                   # based on self.bins. It returns the index starting from 0\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Creating image from the dataset\n",
    "        img_data = self.images_data[idx]\n",
    "        img = np.array(img_data, dtype=np.uint32)\n",
    "\n",
    "        Y_channel = (img[0:1024].reshape(32, 32, 1) / 255).astype(np.float32)\n",
    "        U_channel = (img[1024:2048].reshape(32, 32, 1) / 255).astype(np.float32)\n",
    "        V_channel = (img[2048:].reshape(32, 32, 1) / 255).astype(np.float32)\n",
    "        #return y, u, v, self.filenames[idx]\n",
    "\n",
    "    \n",
    "        #return Y_channel, U_channel, V_channel, self.filenames[idx]\n",
    "\n",
    "        # Binned shapes are (H,W)\n",
    "        U_binned = self.bin_labels(U_channel)  # Discretize U \n",
    "        V_binned = self.bin_labels(V_channel)  # Discretize V\n",
    "\n",
    "        paired_UV = np.stack((U_binned, V_binned), axis=2)\n",
    "\n",
    "        return Y_channel, U_binned, V_binned,paired_UV, self.filenames[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yuv_train_set = unpickle('yuv_train')\n",
    "\n",
    "yuv_images_data = yuv_train_set[b'data']\n",
    "\n",
    "filenames = [f.decode('utf-8') for f in yuv_train_set[b'filenames']]\n",
    "\n",
    "yuv_dataset = AnalysisDataset(filenames=filenames, images_data=yuv_images_data)\n",
    "yuv_train_loader = DataLoader(yuv_dataset, batch_size=32, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.CustomDataset object at 0x11b8a5990>\n"
     ]
    }
   ],
   "source": [
    "# Input tensor(C,H,W) -> output tensor(H,W,C)\n",
    "def rgb_yuv(array):\n",
    "    new_array = array.permute(1,2, 0).numpy().copy()\n",
    "    return torch.Tensor(cv2.cvtColor(new_array, cv2.COLOR_RGB2YUV))\n",
    "\n",
    "# Input tensor(H,W,C) -> output tensor(C,H,W)\n",
    "def yuv_rgb(array):\n",
    "    new_array = array.numpy().copy()\n",
    "    new_array = cv2.cvtColor(new_array, cv2.COLOR_YUV2RGB)\n",
    "    return torch.tensor((new_array*255).astype(np.uint8)).permute(2, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AnalysisDataset(filenames=filenames, images_data=yuv_images_data)\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_U_binned = []\n",
    "all_V_binned = []\n",
    "all_UV_binned = []\n",
    "for batch in train_loader:\n",
    "    image_tensor, filename = batch  # Extract data from the batch\n",
    "    for idx, image_rgb in enumerate(image_tensor):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the bins in the UV map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m \u001b[43mplot_uv_pairs_histogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_UV_binned\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbin_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 32\u001b[0m, in \u001b[0;36mplot_uv_pairs_histogram\u001b[0;34m(all_UV_binned, bin_count)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mplot_uv_pairs_histogram\u001b[39m(all_UV_binned, bin_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m):\n\u001b[1;32m     29\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m    Creates a histogram of binned U,V paired values.\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of UV flat:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mall_UV_binned\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m)\n\u001b[1;32m     33\u001b[0m     UV_flat \u001b[38;5;241m=\u001b[39m all_UV_binned\u001b[38;5;241m.\u001b[39mflatten()  \u001b[38;5;66;03m# Flatten tensor\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m# Compute 2D histogram\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m#hist, x_edges, y_edges = np.histogram2d(U_flat, V_flat, bins=bin_count, range=[[0, bin_count], [0, bin_count]])\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "def plot_uv_histogram(all_U_binned, all_V_binned, bin_count=50):\n",
    "    \"\"\"\n",
    "    Creates a histogram of binned U and V values.\n",
    "    \"\"\"\n",
    "    U_flat = all_U_binned.flatten()  # Flatten tensor\n",
    "    V_flat = all_V_binned.flatten()\n",
    "\n",
    "    # Compute 2D histogram\n",
    "    #hist, x_edges, y_edges = np.histogram2d(U_flat, V_flat, bins=bin_count, range=[[0, bin_count], [0, bin_count]])\n",
    "    bins_u, counts_u = np.unique(U_flat,return_counts=True)[0],np.unique(U_flat,return_counts=True)[1]\n",
    "    bins_v, counts_v = np.unique(V_flat,return_counts=True)[0],np.unique(V_flat,return_counts=True)[1]\n",
    "\n",
    "    # Plot heatmap\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    ax[0].bar(bins_u, counts_u, color='blue')\n",
    "    ax[0].set_xlabel(\"U Bins\")\n",
    "    ax[0].set_ylabel(\"Counts\")\n",
    "    ax[0].set_title(\"Histogram of U values\")\n",
    "\n",
    "    ax[1].bar(bins_v, counts_v, color='red')\n",
    "    ax[1].set_xlabel(\"V Bins\")\n",
    "    ax[1].set_ylabel(\"Counts\")\n",
    "    ax[1].set_title(\"Histogram of V values\")\n",
    "\n",
    "    plt.show()\n",
    "    return bins_u,counts_u,bins_v,counts_v\n",
    "\n",
    "def plot_uv_pairs_histogram(all_UV_binned, bin_count=50):\n",
    "    \"\"\"\n",
    "    Creates a histogram of binned U,V paired values.\n",
    "    \"\"\"\n",
    "    print(\"Shape of UV flat:\", all_UV_binned.shape)\n",
    "    UV_flat = all_UV_binned.flatten()  # Flatten tensor\n",
    "    \n",
    "\n",
    "    # Compute 2D histogram\n",
    "    #hist, x_edges, y_edges = np.histogram2d(U_flat, V_flat, bins=bin_count, range=[[0, bin_count], [0, bin_count]])\n",
    "    bins_uv, counts_uv = np.unique(UV_flat,return_counts=True)[0],np.unique(UV_flat,return_counts=True)[1]\n",
    "\n",
    "    # Plot heatmap\n",
    "    plt.bar(bins_uv, counts_uv, color='purple')\n",
    "    plt.xlabel(\"UV Bins\")\n",
    "    plt.ylabel(\"Counts\")\n",
    "    plt.title(\"Histogram of U values\")\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "# Example usage\n",
    "plot_uv_pairs_histogram(all_UV_binned,bin_count=50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "\n",
    "bins_u,counts_u,bins_v,counts_v = plot_uv_histogram_pytorch(all_U_binned, all_V_binned, bin_count=50)\n",
    "max_U = np.unique(bins_u[np.argmax(counts_u)])\n",
    "max_V = np.unique(bins_v[np.argmax(counts_v)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of bins\n",
    "num_bins = 50\n",
    "Y_fixed=128\n",
    "\n",
    "# Create binned U and V values\n",
    "U_bins = np.linspace(-128, 127, num_bins)\n",
    "V_bins = np.linspace(-128, 127, num_bins)\n",
    "\n",
    "# Create an image grid for binned U and V values\n",
    "UV_binned_grid = np.zeros((num_bins, num_bins, 3), dtype=np.uint8)\n",
    "\n",
    "# Iterate over the binned U, V values and convert to RGB\n",
    "for i, U in enumerate(U_bins):\n",
    "    for j, V in enumerate(V_bins):\n",
    "        # Convert YUV to BGR\n",
    "        YUV_color = np.array([Y_fixed, U + 128, V + 128], dtype=np.uint8)\n",
    "        BGR_color = cv2.cvtColor(YUV_color.reshape(1, 1, 3), cv2.COLOR_YUV2BGR)\n",
    "        UV_binned_grid[j, i] = BGR_color  # Assign to image (swapping j, i)\n",
    "\n",
    "# Display the binned UV color map\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(max_U,max_V,s=50, facecolors='none', edgecolors='black',label=\"Most frequent bin\")\n",
    "plt.imshow(UV_binned_grid, extent=[0, num_bins - 1, 0, num_bins - 1])\n",
    "plt.xlabel(\"U (Cb) Channel\")\n",
    "plt.ylabel(\"V (Cr) Channel\")\n",
    "plt.title(f\"UV Color Map (Y = 128) with {num_bins} Bins\")\n",
    "plt.legend()\n",
    "plt.grid(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the probability distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "bins_u, counts_u = np.unique(all_U_binned.flatten(),return_counts=True)[0],np.unique(all_U_binned.flatten(),return_counts=True)[1]\n",
    "bins_v, counts_v = np.unique(all_V_binned.flatten(),return_counts=True)[0],np.unique(all_V_binned.flatten(),return_counts=True)[1]\n",
    "\n",
    "total_pixel = np.sum(counts_u)\n",
    "full_bins = np.arange(0, num_bins)\n",
    "\n",
    "# Create a dictionary mapping existing bins to their counts\n",
    "bin_count_dict_u = dict(zip(bins_u, counts_u))\n",
    "bin_count_dict_v = dict(zip(bins_v, counts_v))\n",
    "\n",
    "# Ensure all bins have a count (default to 0 for missing bins)\n",
    "full_counts_u = np.array([bin_count_dict_u.get(b, 0) for b in full_bins])\n",
    "full_counts_v = np.array([bin_count_dict_v.get(b, 0) for b in full_bins])\n",
    "\n",
    "probs_u = full_counts_u / np.sum(full_counts_u)\n",
    "probs_v = full_counts_v / np.sum(full_counts_v)\n",
    "\n",
    "'''\n",
    "def u_distrib(bin_count=50):\n",
    "    return np.random.choice(full_bins, bin_count,p=probs_u)\n",
    "\n",
    "def v_distrib(bin_count=50):\n",
    "    return np.random.choice(full_bins, bin_count,p=probs_v)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute weights for U and V binned separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_weights(prob_dist, lambda_param=0.5):\n",
    "    \"\"\"\n",
    "    Compute class rebalancing weights based on the pixel color distribution.\n",
    "    \"\"\"\n",
    "    Q = len(prob_dist)  # Number of quantized bins\n",
    "    uniform_dist = torch.ones(Q) / Q  # Uniform distribution\n",
    "\n",
    "    # Compute the weighted probability\n",
    "    smoothed_prob = (1 - lambda_param) * prob_dist + lambda_param * uniform_dist\n",
    "    weights = 1.0 / smoothed_prob  # Inverse probability\n",
    "    weights /= weights.sum()  # Normalize\n",
    "    \n",
    "    return weights\n",
    "\n",
    "def soft_encode(y_true, bins, sigma=5):\n",
    "    \"\"\"\n",
    "    Perform soft encoding by distributing weights across the nearest neighbors.\n",
    "    - y_true: Ground truth colors (Nx1)\n",
    "    - bins: Discrete ab bins (Qx1)\n",
    "    - sigma: Gaussian kernel standard deviation\n",
    "    \"\"\"\n",
    "    N = y_true.shape[0]\n",
    "    print(\"Y_true shape: \",y_true.shape)\n",
    "    Q = bins.shape[0]\n",
    "    print(\"Q shape: \", bins.shape)\n",
    "    \n",
    "    # Compute Euclidean distance between each pixel and bin\n",
    "    dist = torch.cdist(y_true, bins)  # Shape: (N, Q)\n",
    "    \n",
    "    # Find the 5 nearest neighbors\n",
    "    knn_indices = torch.topk(-dist, k=5, dim=1).indices  # (N, 5)\n",
    "    \n",
    "    # Compute Gaussian weights\n",
    "    knn_dist = torch.gather(dist, 1, knn_indices)  # Distances to 5 nearest neighbors\n",
    "    weights = torch.exp(-knn_dist ** 2 / (2 * sigma ** 2))\n",
    "    weights /= weights.sum(dim=1, keepdim=True)  # Normalize\n",
    "    \n",
    "    # Create soft-encoded one-hot vectors manually\n",
    "    soft_labels = torch.zeros(N, Q, device=y_true.device)\n",
    "    \n",
    "    for i in range(N):\n",
    "        soft_labels[i, knn_indices[i]] += weights[i]\n",
    "    \n",
    "    return soft_labels\n",
    "\n",
    "\n",
    "\n",
    "lambda_param = 0.5\n",
    "weights = compute_weights(torch.Tensor(probs_u), lambda_param)\n",
    "\n",
    "'''\n",
    "# Fake color data (ground truth and bins)\n",
    "y_true = torch.randn(num_samples, 2)  # Ground truth colors\n",
    "bins = torch.randn(num_bins, 2)  # Quantized ab bins\n",
    "\n",
    "# Soft encoding\n",
    "soft_labels = soft_encode(y_true, bins, sigma=5)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "whole_labels = []\n",
    "for img_data in images_data:\n",
    "    img_rgb = np.array(img_data, dtype=np.uint8).reshape(3, 32, 32)\n",
    "    img_rgb = np.transpose(img_rgb, (1, 2, 0))  # Convert to HxWxC format\n",
    "\n",
    "    # Convert RGB to LUV (YUV-like)\n",
    "    image_yuv = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2Luv)\n",
    "    \n",
    "    # Normalize Y to [0,1]\n",
    "    Y_channel = image_yuv[:, :, 0] / 255.0\n",
    "    U_channel = torch.tensor(image_yuv[:, :, 1] / 255.0)\n",
    "    V_channel = image_yuv[:, :, 2] / 255.0\n",
    "\n",
    "    bins = torch.Tensor([np.linspace(0, 1, num_bins + 1),np.linspace(0, 1, num_bins + 1)]) \n",
    "    soft_labels = soft_encode(U_channel,bins, sigma=4)\n",
    "    print(\"Shape soft labels: \",soft_labels.shape)\n",
    "    #print(\"Max value: \",np.max(soft_labels))\n",
    "    whole_labels.append(soft_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute weights for U and V binned together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UV_binned_grid = np.zeros((num_bins, num_bins,2), dtype=np.uint8)\n",
    "\n",
    "# Iterate over the binned U, V values and convert to RGB\n",
    "for i, U in enumerate(U_bins):\n",
    "    for j, V in enumerate(V_bins):\n",
    "        UV_binned_grid[i, j] = np.array([U,V], dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def compute_weights(prob_dist, lambda_param=0.5):\n",
    "    \"\"\"\n",
    "    Compute class rebalancing weights based on the pixel color distribution.\n",
    "    \"\"\"\n",
    "    Q = len(prob_dist)**2  # Number of quantized bins\n",
    "    uniform_dist = torch.ones(Q) / Q  # Uniform distribution\n",
    "\n",
    "    # Compute the weighted probability\n",
    "    smoothed_prob = (1 - lambda_param) * prob_dist + lambda_param * uniform_dist\n",
    "    weights = 1.0 / smoothed_prob  # Inverse probability\n",
    "    weights /= weights.sum()  # Normalize\n",
    "    \n",
    "    return weights\n",
    "\n",
    "def soft_encode(y_true, bins, sigma=5):\n",
    "    \"\"\"\n",
    "    Perform soft encoding by distributing weights across the nearest neighbors.\n",
    "    - y_true: Ground truth colors (Nx2)\n",
    "    - bins: Discrete ab bins (Qx2)\n",
    "    - sigma: Gaussian kernel standard deviation\n",
    "    \"\"\"\n",
    "    N = y_true.shape[0]\n",
    "    print(N)\n",
    "    Q = UV_binned_grid.shape[0]\n",
    "    print(Q)\n",
    "    \n",
    "    # Compute Euclidean distance between each pixel and bin\n",
    "    dist = torch.cdist(y_true, bins)  # Shape: (N, Q)\n",
    "    \n",
    "    # Find the 5 nearest neighbors\n",
    "    knn_indices = torch.topk(-dist, k=5, dim=1).indices  # (N, 5)\n",
    "    \n",
    "    # Compute Gaussian weights\n",
    "    knn_dist = torch.gather(dist, 1, knn_indices)  # Distances to 5 nearest neighbors\n",
    "    weights = torch.exp(-knn_dist ** 2 / (2 * sigma ** 2))\n",
    "    weights /= weights.sum(dim=1, keepdim=True)  # Normalize\n",
    "    \n",
    "    # Create soft-encoded one-hot vectors manually\n",
    "    soft_labels = torch.zeros(N, Q, device=y_true.device)\n",
    "    \n",
    "    for i in range(N):\n",
    "        soft_labels[i, knn_indices[i]] += weights[i]\n",
    "    \n",
    "    return soft_labels\n",
    "\n",
    "\n",
    "\n",
    "lambda_param = 0.5\n",
    "weights = compute_weights(torch.Tensor(probs_u), lambda_param)\n",
    "\n",
    "'''\n",
    "# Fake color data (ground truth and bins)\n",
    "y_true = torch.randn(num_samples, 2)  # Ground truth colors\n",
    "bins = torch.randn(num_bins, 2)  # Quantized ab bins\n",
    "\n",
    "# Soft encoding\n",
    "soft_labels = soft_encode(y_true, bins, sigma=5)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "whole_labels = []\n",
    "for img_data in images_data:\n",
    "    img_rgb = np.array(img_data, dtype=np.uint8).reshape(3, 32, 32)\n",
    "    img_rgb = np.transpose(img_rgb, (1, 2, 0))  # Convert to HxWxC format\n",
    "\n",
    "    # Convert RGB to LUV (YUV-like)\n",
    "    image_yuv = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2Luv)\n",
    "    \n",
    "    # Normalize Y to [0,1]\n",
    "    Y_channel = image_yuv[:, :, 0] / 255.0\n",
    "    U_channel = image_yuv[:, :, 1] / 255.0\n",
    "    V_channel = image_yuv[:, :, 2] / 255.0\n",
    "\n",
    "    bins = torch.Tensor([np.linspace(0, 1, num_bins),np.linspace(0, 1, num_bins)]).T\n",
    "\n",
    "    uv_channels = torch.Tensor([U_channel,V_channel]).T\n",
    "\n",
    "    soft_labels = soft_encode(uv_channels, bins, sigma=4)\n",
    "    print(\"Shape soft labels: \",soft_labels.shape)\n",
    "    #print(\"Max value: \",np.max(soft_labels))\n",
    "    whole_labels.append(soft_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "\n",
    "class ColorizationModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ColorizationModel, self).__init__()\n",
    "        \n",
    "        # Load MobileNetV2 pretrained model (use only convolutional layers)\n",
    "        mobilenet = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1)\n",
    "        self.encoder = mobilenet.features  # Feature extractor\n",
    "        \n",
    "        # Decoder (Lightweight CNN)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(1280, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 2, kernel_size=3, padding=1),  # Output 2-channel (a, b)\n",
    "            nn.Tanh()  # Normalize to [-1, 1] for LAB space\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.encoder(x)  # Extract features from grayscale input\n",
    "        output = self.decoder(features)  # Predict (a, b) channels\n",
    "        return output\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    model = ColorizationModel()\n",
    "    model.eval()\n",
    "    sample_input = torch.randn(1, 1, 32, 32)  # Batch of 1 grayscale image (32x32)\n",
    "    with torch.no_grad():\n",
    "        output = model(sample_input)\n",
    "    print(\"Output shape:\", output.shape)  # Expected (1, 2, 32, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
