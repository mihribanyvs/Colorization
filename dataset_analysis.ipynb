{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "from PIL import Image\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carica il train_set (modifica in base alla posizione del tuo file)\n",
    "train_set = unpickle('train')\n",
    "\n",
    "# Estrai il nome dei file e i dati delle immagini\n",
    "filenames = [f.decode('utf-8') for f in train_set[b'filenames']]  # Decodifica da byte a stringa\n",
    "\n",
    "\n",
    "images_data = train_set[b'data']  # Supponiamo che le immagini siano in un array NumPy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "coarse_labels = np.array(train_set[b'coarse_labels'])\n",
    "f_v_index = list(np.where(coarse_labels == 4)[0])\n",
    "fruits_veggies = [images_data[x] for x in f_v_index]\n",
    "fv_filenames = [filenames[x] for x in f_v_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Crea una classe Dataset personalizzata\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, filenames, images_data, transform=None):\n",
    "        self.filenames = filenames # nomi delle immagini\n",
    "        self.images_data = images_data # nomi dei file \n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Carica l'immagine dal dataset\n",
    "        img_data = self.images_data[idx]\n",
    "        print(img_data)\n",
    "        img = np.array(img_data, dtype=np.uint8).reshape(3, 32, 32)  # (C, H, W) -> RGB, adatta la forma a seconda del tuo caso\n",
    "\n",
    "       \n",
    "        img = Image.fromarray(np.transpose(img, (1, 2, 0)))  # Cambia l'ordine (H, W, C) a (C, H, W)\n",
    "\n",
    "       \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, self.filenames[idx]\n",
    "\n",
    "# Definisci le trasformazioni, ad esempio: ridimensionamento, normalizzazione, ecc.\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Converte l'immagine in un tensor\n",
    "  #  transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalizzazione tipica\n",
    "])\n",
    "\n",
    "\n",
    "dataset = CustomDataset(filenames=fv_filenames, images_data=fruits_veggies, transform=transform)\n",
    "train_loader = DataLoader(dataset, batch_size=2, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.CustomDataset object at 0x11b8a5990>\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 20\u001b[0m\n\u001b[1;32m     16\u001b[0m         plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Supponiamo che 'train_loader' sia già definito\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Carica un batch dal train_loader (ad esempio, il primo batch)\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_tensor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# La forma è (C, H, W)\u001b[39;49;00m\n",
      "File \u001b[0;32m~/Desktop/Colorization/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Desktop/Colorization/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/Desktop/Colorization/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Desktop/Colorization/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[29], line 21\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     17\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(np\u001b[38;5;241m.\u001b[39mtranspose(img, (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m)))  \u001b[38;5;66;03m# Cambia l'ordine (H, W, C) a (C, H, W)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[0;32m---> 21\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilenames[idx]\n",
      "File \u001b[0;32m~/Desktop/Colorization/.venv/lib/python3.11/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/Desktop/Colorization/.venv/lib/python3.11/site-packages/torchvision/transforms/transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[1;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Colorization/.venv/lib/python3.11/site-packages/torchvision/transforms/functional.py:167\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# handle PIL Image\u001b[39;00m\n\u001b[1;32m    166\u001b[0m mode_to_nptype \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mint32, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI;16\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mbyteorder \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlittle\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI;16B\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mint16, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mfloat32}\n\u001b[0;32m--> 167\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_to_nptype\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muint8\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pic\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    170\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;241m*\u001b[39m img\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Numpy is not available"
     ]
    }
   ],
   "source": [
    "# Funzione per visualizzare un'immagine da un tensor\n",
    "def imshow(tensor_img, yuv = False):\n",
    "    # Converti il tensor da (C, H, W) a (H, W, C) per matplotlib\n",
    "    print(tensor_img)\n",
    "    img = tensor_img.numpy()\n",
    "    \n",
    "    if yuv == False:\n",
    "        img = tensor_img.permute(1, 2, 0).numpy()  # (C, H, W) -> (H, W, C)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')  # Nasconde gli assi\n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        plt.imshow(img, cmap = 'jet')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# Supponiamo che 'train_loader' sia già definito\n",
    "# Carica un batch dal train_loader (ad esempio, il primo batch)\n",
    "for batch in train_loader:\n",
    "    image_tensor, filename = batch\n",
    "    image_tensor = batch[0][0]  # La forma è (C, H, W)\n",
    "\n",
    "    # Visualizza l'immagine\n",
    "    print(f\"Filename: {batch[1][0]}\")\n",
    "    imshow(image_tensor)\n",
    "\n",
    "    break  # Visualizza solo la prima immagine del primo batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the color frequencies of the images with the bins created for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def channel_bins(values, bin_count):\n",
    "\n",
    "    bins = np.linspace(0, 1, bin_count + 1)  # Define bin edges\n",
    "    return np.digitize(values, bins) - 1  # Map values to bin indices\n",
    "\n",
    "\n",
    "def pair_bins_grid(value_pairs, bin_count):\n",
    "    bins = np.linspace(0, 1, bin_count + 1)  # Define bin edges\n",
    "    \n",
    "    values1, values2 = value_pairs[0], value_pairs[1]  # Split the pairs\n",
    "    \n",
    "    bin_indices1 = np.digitize(values1, bins) - 1\n",
    "    bin_indices2 = np.digitize(values2, bins) - 1\n",
    "    \n",
    "    return np.stack((bin_indices1, bin_indices2), axis=-1)  # Keep the same shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing the bins as separate and also as pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_images(images_data, bin_count=50):\n",
    "    \"\"\"\n",
    "    Converts images from RGB to YUV, bins the U and V channels, \n",
    "    and concatenates all data into a single tensor.\n",
    "\n",
    "    Args:\n",
    "        images_data (list): List of flattened RGB images.\n",
    "        bin_count (int): Number of bins for U and V channels.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Concatenated YUV data.\n",
    "        torch.Tensor: Concatenated U bins.\n",
    "        torch.Tensor: Concatenated V bins.\n",
    "    \"\"\"\n",
    "\n",
    "    all_Y = []\n",
    "    all_U_binned = []\n",
    "    all_V_binned = []\n",
    "    all_UV_binned = []\n",
    "\n",
    "    for img_data in images_data:\n",
    "        img_rgb = np.array(img_data, dtype=np.uint8).reshape(3, 32, 32)\n",
    "        img_rgb = np.transpose(img_rgb, (1, 2, 0))  # Convert to HxWxC format\n",
    "\n",
    "        # Convert RGB to LUV (YUV-like)\n",
    "        image_yuv = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2Luv)\n",
    "\n",
    "        # Normalize Y to [0,1]\n",
    "        Y_channel = image_yuv[:, :, 0] / 255.0\n",
    "        U_channel = image_yuv[:, :, 1] / 255.0\n",
    "        V_channel = image_yuv[:, :, 2] / 255.0\n",
    "\n",
    "        # Bin U and V channels\n",
    "        U_binned = channel_bins(U_channel,bin_count)\n",
    "        V_binned = channel_bins(V_channel,bin_count)\n",
    "\n",
    "        # Bin U,V pairs\n",
    "        UV_binned = pair_bins_grid([U_channel,V_channel],bin_count) / 255.0\n",
    "\n",
    "        # Store results\n",
    "        all_Y.append(Y_channel[np.newaxis, :, :])  # Add channel dim\n",
    "        all_U_binned.append(U_binned)\n",
    "        all_V_binned.append(V_binned)\n",
    "        all_UV_binned.append(UV_binned)\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    all_Y = np.array(np.stack(all_Y), dtype=np.float32)  # Shape: (N, 1, 32, 32)\n",
    "    all_U_binned = np.array(np.stack(all_U_binned), dtype=np.long)  # (N, 32, 32)\n",
    "    all_V_binned = np.array(np.stack(all_V_binned), dtype=np.long)  # (N, 32, 32)\n",
    "    all_UV_binned = np.array(np.stack(all_V_binned), dtype=np.long) # (N, 2, 32, 32)\n",
    "\n",
    "    return all_Y, all_U_binned, all_V_binned, all_UV_binned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_Y, all_U_binned, all_V_binned, all_UV_binned = process_images(images_data, bin_count=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_UV_binned.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the bins in the UV map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m \u001b[43mplot_uv_pairs_histogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_UV_binned\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbin_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 32\u001b[0m, in \u001b[0;36mplot_uv_pairs_histogram\u001b[0;34m(all_UV_binned, bin_count)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mplot_uv_pairs_histogram\u001b[39m(all_UV_binned, bin_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m):\n\u001b[1;32m     29\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m    Creates a histogram of binned U,V paired values.\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of UV flat:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mall_UV_binned\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m)\n\u001b[1;32m     33\u001b[0m     UV_flat \u001b[38;5;241m=\u001b[39m all_UV_binned\u001b[38;5;241m.\u001b[39mflatten()  \u001b[38;5;66;03m# Flatten tensor\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m# Compute 2D histogram\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m#hist, x_edges, y_edges = np.histogram2d(U_flat, V_flat, bins=bin_count, range=[[0, bin_count], [0, bin_count]])\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "def plot_uv_histogram(all_U_binned, all_V_binned, bin_count=50):\n",
    "    \"\"\"\n",
    "    Creates a histogram of binned U and V values.\n",
    "    \"\"\"\n",
    "    U_flat = all_U_binned.flatten()  # Flatten tensor\n",
    "    V_flat = all_V_binned.flatten()\n",
    "\n",
    "    # Compute 2D histogram\n",
    "    #hist, x_edges, y_edges = np.histogram2d(U_flat, V_flat, bins=bin_count, range=[[0, bin_count], [0, bin_count]])\n",
    "    bins_u, counts_u = np.unique(U_flat,return_counts=True)[0],np.unique(U_flat,return_counts=True)[1]\n",
    "    bins_v, counts_v = np.unique(V_flat,return_counts=True)[0],np.unique(V_flat,return_counts=True)[1]\n",
    "\n",
    "    # Plot heatmap\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    ax[0].bar(bins_u, counts_u, color='blue')\n",
    "    ax[0].set_xlabel(\"U Bins\")\n",
    "    ax[0].set_ylabel(\"Counts\")\n",
    "    ax[0].set_title(\"Histogram of U values\")\n",
    "\n",
    "    ax[1].bar(bins_v, counts_v, color='red')\n",
    "    ax[1].set_xlabel(\"V Bins\")\n",
    "    ax[1].set_ylabel(\"Counts\")\n",
    "    ax[1].set_title(\"Histogram of V values\")\n",
    "\n",
    "    plt.show()\n",
    "    return bins_u,counts_u,bins_v,counts_v\n",
    "\n",
    "def plot_uv_pairs_histogram(all_UV_binned, bin_count=50):\n",
    "    \"\"\"\n",
    "    Creates a histogram of binned U,V paired values.\n",
    "    \"\"\"\n",
    "    print(\"Shape of UV flat:\", all_UV_binned.shape)\n",
    "    UV_flat = all_UV_binned.flatten()  # Flatten tensor\n",
    "    \n",
    "\n",
    "    # Compute 2D histogram\n",
    "    #hist, x_edges, y_edges = np.histogram2d(U_flat, V_flat, bins=bin_count, range=[[0, bin_count], [0, bin_count]])\n",
    "    bins_uv, counts_uv = np.unique(UV_flat,return_counts=True)[0],np.unique(UV_flat,return_counts=True)[1]\n",
    "\n",
    "    # Plot heatmap\n",
    "    plt.bar(bins_uv, counts_uv, color='purple')\n",
    "    plt.xlabel(\"UV Bins\")\n",
    "    plt.ylabel(\"Counts\")\n",
    "    plt.title(\"Histogram of U values\")\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "# Example usage\n",
    "plot_uv_pairs_histogram(all_UV_binned,bin_count=50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "\n",
    "bins_u,counts_u,bins_v,counts_v = plot_uv_histogram_pytorch(all_U_binned, all_V_binned, bin_count=50)\n",
    "max_U = np.unique(bins_u[np.argmax(counts_u)])\n",
    "max_V = np.unique(bins_v[np.argmax(counts_v)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of bins\n",
    "num_bins = 50\n",
    "Y_fixed=128\n",
    "\n",
    "# Create binned U and V values\n",
    "U_bins = np.linspace(-128, 127, num_bins)\n",
    "V_bins = np.linspace(-128, 127, num_bins)\n",
    "\n",
    "# Create an image grid for binned U and V values\n",
    "UV_binned_grid = np.zeros((num_bins, num_bins, 3), dtype=np.uint8)\n",
    "\n",
    "# Iterate over the binned U, V values and convert to RGB\n",
    "for i, U in enumerate(U_bins):\n",
    "    for j, V in enumerate(V_bins):\n",
    "        # Convert YUV to BGR\n",
    "        YUV_color = np.array([Y_fixed, U + 128, V + 128], dtype=np.uint8)\n",
    "        BGR_color = cv2.cvtColor(YUV_color.reshape(1, 1, 3), cv2.COLOR_YUV2BGR)\n",
    "        UV_binned_grid[j, i] = BGR_color  # Assign to image (swapping j, i)\n",
    "\n",
    "# Display the binned UV color map\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(max_U,max_V,s=50, facecolors='none', edgecolors='black',label=\"Most frequent bin\")\n",
    "plt.imshow(UV_binned_grid, extent=[0, num_bins - 1, 0, num_bins - 1])\n",
    "plt.xlabel(\"U (Cb) Channel\")\n",
    "plt.ylabel(\"V (Cr) Channel\")\n",
    "plt.title(f\"UV Color Map (Y = 128) with {num_bins} Bins\")\n",
    "plt.legend()\n",
    "plt.grid(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the probability distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "bins_u, counts_u = np.unique(all_U_binned.flatten(),return_counts=True)[0],np.unique(all_U_binned.flatten(),return_counts=True)[1]\n",
    "bins_v, counts_v = np.unique(all_V_binned.flatten(),return_counts=True)[0],np.unique(all_V_binned.flatten(),return_counts=True)[1]\n",
    "\n",
    "total_pixel = np.sum(counts_u)\n",
    "full_bins = np.arange(0, num_bins)\n",
    "\n",
    "# Create a dictionary mapping existing bins to their counts\n",
    "bin_count_dict_u = dict(zip(bins_u, counts_u))\n",
    "bin_count_dict_v = dict(zip(bins_v, counts_v))\n",
    "\n",
    "# Ensure all bins have a count (default to 0 for missing bins)\n",
    "full_counts_u = np.array([bin_count_dict_u.get(b, 0) for b in full_bins])\n",
    "full_counts_v = np.array([bin_count_dict_v.get(b, 0) for b in full_bins])\n",
    "\n",
    "probs_u = full_counts_u / np.sum(full_counts_u)\n",
    "probs_v = full_counts_v / np.sum(full_counts_v)\n",
    "\n",
    "'''\n",
    "def u_distrib(bin_count=50):\n",
    "    return np.random.choice(full_bins, bin_count,p=probs_u)\n",
    "\n",
    "def v_distrib(bin_count=50):\n",
    "    return np.random.choice(full_bins, bin_count,p=probs_v)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute weights for U and V binned separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_weights(prob_dist, lambda_param=0.5):\n",
    "    \"\"\"\n",
    "    Compute class rebalancing weights based on the pixel color distribution.\n",
    "    \"\"\"\n",
    "    Q = len(prob_dist)  # Number of quantized bins\n",
    "    uniform_dist = torch.ones(Q) / Q  # Uniform distribution\n",
    "\n",
    "    # Compute the weighted probability\n",
    "    smoothed_prob = (1 - lambda_param) * prob_dist + lambda_param * uniform_dist\n",
    "    weights = 1.0 / smoothed_prob  # Inverse probability\n",
    "    weights /= weights.sum()  # Normalize\n",
    "    \n",
    "    return weights\n",
    "\n",
    "def soft_encode(y_true, bins, sigma=5):\n",
    "    \"\"\"\n",
    "    Perform soft encoding by distributing weights across the nearest neighbors.\n",
    "    - y_true: Ground truth colors (Nx1)\n",
    "    - bins: Discrete ab bins (Qx1)\n",
    "    - sigma: Gaussian kernel standard deviation\n",
    "    \"\"\"\n",
    "    N = y_true.shape[0]\n",
    "    print(\"Y_true shape: \",y_true.shape)\n",
    "    Q = bins.shape[0]\n",
    "    print(\"Q shape: \", bins.shape)\n",
    "    \n",
    "    # Compute Euclidean distance between each pixel and bin\n",
    "    dist = torch.cdist(y_true, bins)  # Shape: (N, Q)\n",
    "    \n",
    "    # Find the 5 nearest neighbors\n",
    "    knn_indices = torch.topk(-dist, k=5, dim=1).indices  # (N, 5)\n",
    "    \n",
    "    # Compute Gaussian weights\n",
    "    knn_dist = torch.gather(dist, 1, knn_indices)  # Distances to 5 nearest neighbors\n",
    "    weights = torch.exp(-knn_dist ** 2 / (2 * sigma ** 2))\n",
    "    weights /= weights.sum(dim=1, keepdim=True)  # Normalize\n",
    "    \n",
    "    # Create soft-encoded one-hot vectors manually\n",
    "    soft_labels = torch.zeros(N, Q, device=y_true.device)\n",
    "    \n",
    "    for i in range(N):\n",
    "        soft_labels[i, knn_indices[i]] += weights[i]\n",
    "    \n",
    "    return soft_labels\n",
    "\n",
    "\n",
    "\n",
    "lambda_param = 0.5\n",
    "weights = compute_weights(torch.Tensor(probs_u), lambda_param)\n",
    "\n",
    "'''\n",
    "# Fake color data (ground truth and bins)\n",
    "y_true = torch.randn(num_samples, 2)  # Ground truth colors\n",
    "bins = torch.randn(num_bins, 2)  # Quantized ab bins\n",
    "\n",
    "# Soft encoding\n",
    "soft_labels = soft_encode(y_true, bins, sigma=5)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "whole_labels = []\n",
    "for img_data in images_data:\n",
    "    img_rgb = np.array(img_data, dtype=np.uint8).reshape(3, 32, 32)\n",
    "    img_rgb = np.transpose(img_rgb, (1, 2, 0))  # Convert to HxWxC format\n",
    "\n",
    "    # Convert RGB to LUV (YUV-like)\n",
    "    image_yuv = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2Luv)\n",
    "    \n",
    "    # Normalize Y to [0,1]\n",
    "    Y_channel = image_yuv[:, :, 0] / 255.0\n",
    "    U_channel = torch.tensor(image_yuv[:, :, 1] / 255.0)\n",
    "    V_channel = image_yuv[:, :, 2] / 255.0\n",
    "\n",
    "    bins = torch.Tensor([np.linspace(0, 1, num_bins + 1),np.linspace(0, 1, num_bins + 1)]) \n",
    "    soft_labels = soft_encode(U_channel,bins, sigma=4)\n",
    "    print(\"Shape soft labels: \",soft_labels.shape)\n",
    "    #print(\"Max value: \",np.max(soft_labels))\n",
    "    whole_labels.append(soft_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute weights for U and V binned together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UV_binned_grid = np.zeros((num_bins, num_bins,2), dtype=np.uint8)\n",
    "\n",
    "# Iterate over the binned U, V values and convert to RGB\n",
    "for i, U in enumerate(U_bins):\n",
    "    for j, V in enumerate(V_bins):\n",
    "        UV_binned_grid[i, j] = np.array([U,V], dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def compute_weights(prob_dist, lambda_param=0.5):\n",
    "    \"\"\"\n",
    "    Compute class rebalancing weights based on the pixel color distribution.\n",
    "    \"\"\"\n",
    "    Q = len(prob_dist)**2  # Number of quantized bins\n",
    "    uniform_dist = torch.ones(Q) / Q  # Uniform distribution\n",
    "\n",
    "    # Compute the weighted probability\n",
    "    smoothed_prob = (1 - lambda_param) * prob_dist + lambda_param * uniform_dist\n",
    "    weights = 1.0 / smoothed_prob  # Inverse probability\n",
    "    weights /= weights.sum()  # Normalize\n",
    "    \n",
    "    return weights\n",
    "\n",
    "def soft_encode(y_true, bins, sigma=5):\n",
    "    \"\"\"\n",
    "    Perform soft encoding by distributing weights across the nearest neighbors.\n",
    "    - y_true: Ground truth colors (Nx2)\n",
    "    - bins: Discrete ab bins (Qx2)\n",
    "    - sigma: Gaussian kernel standard deviation\n",
    "    \"\"\"\n",
    "    N = y_true.shape[0]\n",
    "    print(N)\n",
    "    Q = UV_binned_grid.shape[0]\n",
    "    print(Q)\n",
    "    \n",
    "    # Compute Euclidean distance between each pixel and bin\n",
    "    dist = torch.cdist(y_true, bins)  # Shape: (N, Q)\n",
    "    \n",
    "    # Find the 5 nearest neighbors\n",
    "    knn_indices = torch.topk(-dist, k=5, dim=1).indices  # (N, 5)\n",
    "    \n",
    "    # Compute Gaussian weights\n",
    "    knn_dist = torch.gather(dist, 1, knn_indices)  # Distances to 5 nearest neighbors\n",
    "    weights = torch.exp(-knn_dist ** 2 / (2 * sigma ** 2))\n",
    "    weights /= weights.sum(dim=1, keepdim=True)  # Normalize\n",
    "    \n",
    "    # Create soft-encoded one-hot vectors manually\n",
    "    soft_labels = torch.zeros(N, Q, device=y_true.device)\n",
    "    \n",
    "    for i in range(N):\n",
    "        soft_labels[i, knn_indices[i]] += weights[i]\n",
    "    \n",
    "    return soft_labels\n",
    "\n",
    "\n",
    "\n",
    "lambda_param = 0.5\n",
    "weights = compute_weights(torch.Tensor(probs_u), lambda_param)\n",
    "\n",
    "'''\n",
    "# Fake color data (ground truth and bins)\n",
    "y_true = torch.randn(num_samples, 2)  # Ground truth colors\n",
    "bins = torch.randn(num_bins, 2)  # Quantized ab bins\n",
    "\n",
    "# Soft encoding\n",
    "soft_labels = soft_encode(y_true, bins, sigma=5)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "whole_labels = []\n",
    "for img_data in images_data:\n",
    "    img_rgb = np.array(img_data, dtype=np.uint8).reshape(3, 32, 32)\n",
    "    img_rgb = np.transpose(img_rgb, (1, 2, 0))  # Convert to HxWxC format\n",
    "\n",
    "    # Convert RGB to LUV (YUV-like)\n",
    "    image_yuv = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2Luv)\n",
    "    \n",
    "    # Normalize Y to [0,1]\n",
    "    Y_channel = image_yuv[:, :, 0] / 255.0\n",
    "    U_channel = image_yuv[:, :, 1] / 255.0\n",
    "    V_channel = image_yuv[:, :, 2] / 255.0\n",
    "\n",
    "    bins = torch.Tensor([np.linspace(0, 1, num_bins),np.linspace(0, 1, num_bins)]).T\n",
    "\n",
    "    uv_channels = torch.Tensor([U_channel,V_channel]).T\n",
    "\n",
    "    soft_labels = soft_encode(uv_channels, bins, sigma=4)\n",
    "    print(\"Shape soft labels: \",soft_labels.shape)\n",
    "    #print(\"Max value: \",np.max(soft_labels))\n",
    "    whole_labels.append(soft_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "\n",
    "class ColorizationModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ColorizationModel, self).__init__()\n",
    "        \n",
    "        # Load MobileNetV2 pretrained model (use only convolutional layers)\n",
    "        mobilenet = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1)\n",
    "        self.encoder = mobilenet.features  # Feature extractor\n",
    "        \n",
    "        # Decoder (Lightweight CNN)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(1280, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 2, kernel_size=3, padding=1),  # Output 2-channel (a, b)\n",
    "            nn.Tanh()  # Normalize to [-1, 1] for LAB space\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.encoder(x)  # Extract features from grayscale input\n",
    "        output = self.decoder(features)  # Predict (a, b) channels\n",
    "        return output\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    model = ColorizationModel()\n",
    "    model.eval()\n",
    "    sample_input = torch.randn(1, 1, 32, 32)  # Batch of 1 grayscale image (32x32)\n",
    "    with torch.no_grad():\n",
    "        output = model(sample_input)\n",
    "    print(\"Output shape:\", output.shape)  # Expected (1, 2, 32, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
