{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotting import imshow\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RGB training data load\n",
    "yuv_train_set = unpickle('yuv_train')\n",
    "\n",
    "# Image names \n",
    "filenames = [f.decode('utf-8') for f in yuv_train_set[b'filenames']]\n",
    "\n",
    "# Getting the images\n",
    "yuv_images_data = yuv_train_set[b'data'].copy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yuv_rgb(array):\n",
    "    new_array = array.numpy().copy()\n",
    "    new_array = cv2.cvtColor(new_array, cv2.COLOR_YUV2RGB)\n",
    "    return torch.tensor((new_array*255).astype(np.uint8)).permute(2, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YUVImageClassificationDataset(Dataset):\n",
    "    def __init__(self, filenames, images_data, bin_count=50):\n",
    "        self.filenames = filenames # image name\n",
    "        self.images_data = images_data # images data: data used to construct the images \n",
    "        self.bin_count = bin_count # number of bins used \n",
    "        self.bins = np.linspace(0, 1, bin_count)  # Bin edges\n",
    "        # we are creating bins between 0 and 1 in order to have the values of u and v\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "    \n",
    "    def bin_labels(self, values):\n",
    "        return np.digitize(values, self.bins) -1 # Map values to bin indices\n",
    "                                                   # This function takes values and assigns each value to a bin index\n",
    "                                                   # based on self.bins. It returns the index starting from 0\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Creating image from the dataset\n",
    "        img_data = self.images_data[idx]\n",
    "        img = np.array(img_data, dtype=np.uint8).reshape(3,32,32)  # (C, H, W)\n",
    "        img = np.transpose(img, (1, 2, 0))  # (H, W, C)\n",
    "\n",
    "        # Channel shapes are (H,W)\n",
    "        Y_channel = img[:, :, 0] / 255.0  # Normalize Y to [0, 1] for stability of the network \n",
    "        U_channel = img[:, :, 1] / 255.0   \n",
    "        V_channel = img[:, :, 2] / 255.0\n",
    "\n",
    "        # Binned shapes are (H,W)\n",
    "        U_binned = self.bin_labels(U_channel)  # Discretize U \n",
    "        V_binned = self.bin_labels(V_channel)  # Discretize V\n",
    "        \n",
    "        # Expanded (1,H,W)\n",
    "        Y_channel = np.expand_dims(Y_channel, axis=0)\n",
    "\n",
    "        # Tensor shapes are the same as the array shapes\n",
    "        Y_channel = torch.tensor(Y_channel, dtype=torch.float32) # transform into tensor to use pytorch \n",
    "        U_binned = torch.tensor(U_binned, dtype=torch.int)\n",
    "        V_binned = torch.tensor(V_binned, dtype=torch.int)\n",
    "       \n",
    "        return Y_channel, U_binned, V_binned, self.filenames[idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a lookup table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_count = 50\n",
    "bins = np.linspace(0, 1, bin_count)\n",
    "lookup_table = torch.zeros(50) \n",
    "for idx, value in enumerate(bins):\n",
    "    lookup_table[idx] = value\n",
    "\n",
    "def unbin_labels(bin_indices):\n",
    "    return lookup_table[bin_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 89, 112,  51], dtype=torch.uint8)\n",
      "Filename: bos_taurus_s_000507.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3t/5nmppwwx2p7b5hfgq7bzhzc00000gn/T/ipykernel_9813/1062105336.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  U_target_rec_tensor = torch.tensor(U_target_rec, dtype=torch.float32).resize(8,1,32,32)\n",
      "/Users/mihriban/Desktop/Colorization/.venv/lib/python3.11/site-packages/torch/_tensor.py:868: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "/var/folders/3t/5nmppwwx2p7b5hfgq7bzhzc00000gn/T/ipykernel_9813/1062105336.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  V_target_rec_tensor = torch.tensor(V_target_rec , dtype=torch.float32).resize(8,1,32,32)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAFs1JREFUeJzt3N2PnId1H+B3P2bIndnVLj+WkkiJpGTJrimrdGq5qRLLsRM0QZoARQsUKIpeFe1F/5NeFmgL9C5AL3pT1Cji1kZcJ7VcW4osuQplm1QkkuJK4orkkstZ7s5wd2Z3pxgBPbc+p+AkCvw810dH774f8+N78f5mxuPxuAGApmlm/7oPAIDPDqEAQBAKAAShAEAQCgAEoQBAEAoABKEAQJhvksbNYXYUfiWMD/bSs7fWrpV2v/GT/5Oe/frf/73S7uMnVptfBQdN/rvcwUHt9217ZzM9e+3q5dLu4ycX07NrH75f2v0Hr/yzXzrjTQGAIBQACEIBgCAUAAhCAYAgFAAIQgGAIBQACEIBgCAUAAhCAYB691HVeJzvHYHPgsODUWl+ZpTvv9m+U+s++sG3v5We3dneLe3+5//qX+aHD2vP8eFhvkNoPDfTTNPoID+7fmuttHvz/np69tbHV0q7r1+9k57d2rpf2t3oPgKgQigAEIQCAEEoABCEAgBBKAAQhAIAQSgAEIQCAEEoADD9mouKmZnpfu7Or6bDplbRMDtTmx8fbOeHd++VdncO99Kz9259Utp9+5Nb6dm52dq/Gx9bXk7Ptmfbpd0HxX/Djsf76dn5Vml1Mzzop2dPnDpW2j3a2EjP3rqWv5ZZ3hQACEIBgCAUAAhCAYAgFAAIQgGAIBQACEIBgCAUAAhCAYAgFAD4bHUf/aqodPGMD4el3aPNu6X53a18b8/4SLe0+7EzZ/LD1d6rcf4czh6OSqu3P/mwNP/Bz9/Iz155t7R7bvZIevbBJx+Xdv/gO99Kzx47c7a0++Xf+Fp6dma+1n10r3e/NL+7k++E2t2rPT/j/Z307O3ND0q7e1v5Yzk8aB45bwoABKEAQBAKAAShAEAQCgAEoQBAEAoABKEAQBAKAAShAEBQc/FXaHxwmJ7duFarRdj46Y9L8/17vfTs7WHt3w7Pv/KN9OznL36ltHu2nb9lf/bzn5d2v/2DPyvN76zn6yW27twq7Z6fy1dAPLx3s7T7z/7HjfTshW/8bmn3y1//Znp2d7hX2n1/I3/cE9fe/E569vb69dLuE2efTs8ODgel3aN+Kz17ZO5U86h5UwAgCAUAglAAIAgFAIJQACAIBQCCUAAgCAUAglAAIAgFAIJQACDoPvorNN7Ld73ce/dabXdvqzR/fP4gPTuz/7C0+/r//l56dn48U9q9cPpsevY//ddvl3b/4s1Lpflnjy+kZ4/P7Jd2d1v5/pvDQk/SxAfv5buSfvz+t0q7n3jqS+nZV776xdLuO1deL82/873/lp7d7W2WdvfPX0jPdi+8VNrd6ZxIzy4+c7x51LwpABCEAgBBKAAQhAIAQSgAEIQCAEEoABCEAgBBKAAQhAIA06+5qLQX1IoO/uaabeXrCBYfP13afffmjdL83p2P07Od9mFpd3s3P/vun/+otHtw/Hx69nt/8uPa7u2d0vzS7JP52WO1Kor+Xr4W490PPynt/mQn/8R9dK9W//Cf/+iP8rvfPlXa/fDjn5bmu/v99OyRhXxlycRwkK9+Ob90sqmYefz59OzeTL4OJcubAgBBKAAQhAIAQSgAEIQCAEEoABCEAgBBKAAQhAIAQSgAEIQCANPvPpoZV4andRRN0xyOizE5xYNpzaVHn3jxYmn1aPt+af76h++mZ9v3N2rH0s73yLz/3uXS7u3FQXp2flTrbNrevFeaf3Cim55dOJfvs5nY6vXSs++s1bqPNvaOpmeXlpdKu29cvZSePbKZ7w+aeH611h/VbuWf/d5e7XfisVP5e/zmzY9Ku1cWjqdnWydqvUoZ3hQACEIBgCAUAAhCAYAgFAAIQgGAIBQACEIBgCAUAAhCAYDp11xU6iUO8+0PZTPj2ufrM4fTq8SYOcgfS6tQFTFx5td/szTftPLHfuvtH5dWP/XkU+nZu3drVRSX3ng7Pbswv1vafWKxVqPwja+/nJ799YsvlHb/u//w79Oz24NRaXf7SCc/vL9T2r3bz9eQHDmbr3OYODwcluZv39lKz7ZWnijtnumupmff+cW10u6tn15Jz57+3LOl3f/km7/7S2e8KQAQhAIAQSgAEIQCAEEoABCEAgBBKAAQhAIAQSgAEIQCAEEoAFDvPjo8rHXUVFqBxsV+ot29vfRsu9Uq7Z6dyefkXOmvrHUlFWqSPnV9c7M0f//I0fTs3vO13p4LL30tPTtcu1va/V++8/307Ohhv7T7H/3+b5Xm//Ef/l569r2r10u7b/fzz9twXKswazf76dnWfK2YbHHhSH52Od8fNLE1yvcqfbr/VL7PaLywXNr98Ua+E+rw4cPS7uHWg/Ts//rjPy7tbv7Nv/2lI94UAAhCAYAgFAAIQgGAIBQACEIBgCAUAAhCAYAgFAAIQgGAIBQACOnSlL3RsKk42s53oDzob5d2/+jNN9Kzjy0ulXb/nRdeTM8udbql3QcHB+nZjzfWS7t/8KP/WZq//uFaenb4sHbt26efSc/ub++Wdt++kT/une1a99Gz586W5ueb/PXsbW2Vdg8P851D+4f545gY7X2Snt07rHWHnTjaSc/evF+7xzc2NkrzrbmV9OzKSu03qHM8v/vY3Ki0uzWb7z46daL2+5bhTQGAIBQACEIBgCAUAAhCAYAgFAAIQgGAIBQACEIBgCAUAKjXXIyOjJuKhw8207Ovvf1WaffV9Rvp2VarXdq9tJKvrrjw7HOl3VtbvfTsm+/8sLT7xo1flOY/Wsufw42t2mf6n/uLk+nZL5+tncOzZ46nZ3u7R0u7F1ZPleav3f4oPfvRrWul3Zv9m+nZhaXa9ekP8rNbvVo9R+vUcnp25Ui+EmNivzg/HOVrTvZ3alUu+zP5yo3+yunS7vHRfLXIcu0WT/GmAEAQCgAEoQBAEAoABKEAQBAKAAShAEAQCgAEoQBAEAoABKEAQL37aGa31q/y6k9eS8+++fN3Sruf+8KT6dn1j/N9QxPf/c6r+eF/UFrdXL1+KT17o9BN9P9j43a+AGf91vul3a3xS+nZL5+rdR/963/xT9OzvWJvz/nHat066zfX07PXfna5tHuwmb9vV5ZXSruHo/yzfKpVOydnTua7w5rZwdSOu6p1tLZ7VBjv727UjqXQ1zbay/ckZXlTACAIBQCCUAAgCAUAglAAIAgFAIJQACAIBQCCUAAgCAUA6jUXW71aXcSrP/xhenb5idXS7lHhG/O1D2qfmFe8dqlQidE0zeUrtaqDivZB8T+Y7adHf+sb+dqKidXV/PWs1qdceOb59OxsL/83Tnz8J39amu9u5Z+J3146V9p94pn8sb99625pd7+Tr644/2Tt2Tw1n69d6O/WflOqKr8To2Ftd6ebP4ej7VqdR7vQLNI62jxy3hQACEIBgCAUAAhCAYAgFAAIQgGAIBQACEIBgCAUAAhCAYAgFACodx/NL3WbipVj+Q6Um7feL+2+dCnfJbJ2tdY7cvpc/rhXb/en1sWy1dsq7e7O5o974vnPn07Prj5eu/aDfv689PuDqZ3DvQ+vlnY/uLFemy90Hx1ZKRTaNE3zd5/OX58zCyul3Z3b+b9z4WTtuIdzhS6ral9XM71+r6ap3ePTVLnHa099jjcFAIJQACAIBQCCUAAgCAUAglAAIAgFAIJQACAIBQCCUACgXnPx2k8vNxXDmcpn4xul3Wtra+nZ9Vv5KoKJ7rHl9OxoeLK0e1CodOhtDku7V88t1+ZP5Y997aMrtd3dfEVD67FajcKRT/L1H+vv1OpT3tuu1Zb89/cupWf7B6NaTczR/Hn55ucvlnZ/7XT++ty4VasKqVzPzYVatcR+P//cT8wUZoet2vXp7ubnZ/Zrv0GjplZb8qh5UwAgCAUAglAAIAgFAIJQACAIBQCCUAAgCAUAglAAIAgFAIJQAKDefbT2Ua37qJnN98icObZaWv1Wk+/iabVbpd2/89v/MD174YVzpd1v7efP4eqJWhfL+cfzfTYTJ1fz5/zc2Qul3ZXr2T4orW42P8z3GfW210u7r83ku6kmFv92vnNof6+2+9atfB/Yt6/X+oleOJO/Pp8b17qpmlv5+3Z0opmq0Wg/Pdsp3oejYf7v7O8+LO1uzRVmW7XfzgxvCgAEoQBAEAoABKEAQBAKAAShAEAQCgAEoQBAEAoABKEAQBAKANS7j04/22sqVk8sp2dHo1rPz++3v5Ke3bhd65zpLjZTO+6Lv/Z8fvd2vjtqYv3jrdL8S4WupPPnXi7t3tjI9/bc3MzPTtxfz/cZHZ49X9r9yiu1LqvhTDc9e3+vdj0r99aVy7Veso+ur6VnT7RLq5vjB/musdGw9mxO02g0nOLuUWl+PFpIz7batd/lDG8KAAShAEAQCgAEoQBAEAoABKEAQBAKAAShAEAQCgAEoQBAvebi9Te+31QMC592n39mtbT74le/mJ5du1H8DPzwRnq09/BOafVomD8n/Z3S6ubu/VrNxWs/y5+XS9c6U6vcaPdrFQAvLpxMz45XarUVt4vH8upPvpsfPqidw1Y7Xxex9SBf/TExLFRX7MzXjrvp5mtLhuOZ0urRaL+Zllar/ZmpxWi3KlUhtXs2w5sCAEEoABCEAgBBKAAQhAIAQSgAEIQCAEEoABCEAgBBKAAQhAIA9e6jc8/W+olG+/lunZOPj0u7eztX87Nbg9LuVis/PzzId5RMDAb53aNhrVtl+ana9WkfyV+f1mKtP+rc3yoMH9a6dZYKXTyvvna5tPvSlbXS/PLKSn64VeuoGe3mZ+/s1HqvKg6fKPyNk2fidr77aKfwGzHRas0X51tT6RuaGAz6TVbrSG130yw0f528KQAQhAIAQSgAEIQCAEEoABCEAgBBKAAQhAIAQSgAEIQCACH93fhXX3yuqegXKh2uvPtWafednfzn8RdefKm0u9PtNtOysZGvABjtLZd293q1yoDeVr4aYfnkydLu1dV85cZgu7S66bTztQvjwuyndvP1KRPtg/zsynLteg4L4xubtRqF5dVz+dmjtRqS7bvv54cP81UR1dqKquGoWEMy2k/Pdhbnp3YsK5WqlSRvCgAEoQBAEAoABKEAQBAKAAShAEAQCgAEoQBAEAoABKEAQBAKAIR0Kcf9Ub6351Nz+Q6h3r1ap8nlv8wfy9Urf1rafe7ZfM/PxS8/X9p9+ql8oc2xVr4/aGI4Ol2aHxX6VbqdWv9NRafTLs2f7OTP+W/8Wq0X5tRSbf71115Lz24Vu6kq/Td3P1ov7a50Qh0+uTq1Pqh2u/bcd+dq/UT9wcP07OggPzvRWWymZmY336s0szt85P9/bwoABKEAQBAKAAShAEAQCgAEoQBAEAoABKEAQBAKAAShAEC95qI7W/skvZnNf3798m9eLK0++4Xn0rNra2ul3Rv38vMbd2vVH63CZ/0b/cul3SsrtYqGleX8/LAZlHb3B/307KmlWj3H6vF8DUnv6do9+/rrr5fmNzbuNp8F46O1v3P18Xxtyclz+WqWibW5ZmrGC8VajClVYlSNRvnaiqr7u7VnM8ObAgBBKAAQhAIAQSgAEIQCAEEoABCEAgBBKAAQhAIAQSgAEIQCAPXuo2a21rHRLvT8dLuVlpKmWV09n5698KX87MSgn+/tqZ6TSlfS1tZWbfdgvTS/OlrNzy7WepVG4156tv+w1mfT2833E63fzR/HxKVffH9qf2er3W6mpbtYO4dnTua7j7Ye1O6r+aX8s7w8NyrtPpypzY9a+fnxYW33oNCVVO2NGxb6o4qNdCneFAAIQgGAIBQACEIBgCAUAAhCAYAgFAAIQgGAIBQACEIBgDAzHo/HTcJ//NHfaypWVvLVCJ2j+cqFiW43/5n+ypH8bNlhrZ7jzna+FmGwM1Pa3XtwszTfPlr7rL9iYyNf57FxezC1iobzJy6Wdj/8oHY93/nzt9Kzw9Foas/PznztHp+9nz/n792t1Vx8YX45PTvu1M7Jxk6ttqTXq1XFVGz18sfS23pQ2t3pLORnT9QqaL793V/+bHpTACAIBQCCUAAgCAUAglAAIAgFAIJQACAIBQCCUAAgCAUAglAAIMw3Sb29WkdNfzPfa9Lp9Eu7j+3mO2p6c/kulqpOp9aV0+3mO56OtWt9Nqcea5Xmh6NhenZrq9Yh89bVtfRsq1077r+4kt+9frRd2v35oxdK86tL+d6Z1RPnSrvb4/zzttetncONdr6baqep3Yfz7ULnWae2e3ysNj8ajqbWTdUq3LetVu36tOdOpmdXlk43j5o3BQCCUAAgCAUAglAAIAgFAIJQACAIBQCCUAAgCAUAglAAIAgFAOrdR+dXn2sqRrv53pnhfK37aLSXn93YrPX2DPr5zpnTT9d6lQZH87P7D2pdU51urRdmdTXfw9Ru1TqEzp3P9/x0Fkurm7UP8j0y83O1c3L4eL4PamJpNX/9t3u9qXUfPffiF0u7h3+Z/zvP7NV6e7qFPrDhXGl1s9rUusYqBvdqvxPNYXcqHUwTrfnW1LrDMrwpABCEAgBBKAAQhAIAQSgAEIQCAEEoABCEAgBBKAAQhAIA9ZqL0aj2qfa48EX6aut0aXflWPpzxbqIhcKBH9Q+Me8Vqg6q1RILR2vHUqkY6O3VakhWT3WnVs9x+nT+Xqnes6ODWhXF6vHz6dn+oHYOu7P5Co35bu3vXLi7kp7tHDtW2j2uDO/XzndrPn/cE52j+SqXfnumtLtb+Jm4cbv2LHcX8vdK/+FG86h5UwAgCAUAglAAIAgFAIJQACAIBQCCUAAgCAUAglAAIAgFAIJQAKDefVTt2Gjt57t41kfrtd1zlQ6UWl/KynK+c6Y/KJ6TVv6ctNq1LqPBYDC1+X6/1ttT7RyqqJyXapdR+6B2LKN+/vrX2m8m1yf/d/ZGxf6bxXxxz3ix1gn0cCN/7YejYWn3cK52H+7vFs7LbO35uXE7/5t1d+NuaXf7yGPp2d5e7bgzvCkAEIQCAEEoABCEAgBBKAAQhAIAQSgAEIQCAEEoABCEAgD1movBg/yn8ROj8c388GFt93CY/8S8XftKv1lbu5Ge7T2o1XNc+NK59OzWdm139RxWjIa12oobHxTqHwrVHxMnH++kZ7tLpdXN8rHasYwLdR6dTv64J7Z28rt3dmvXZ383f289nK89QPsHlVqZ2jkZHWzV5lv5CohBpRKjaZq1Gx+mZ3v3a9dn+XR+fnm2Vv2R4U0BgCAUAAhCAYAgFAAIQgGAIBQACEIBgCAUAAhCAYAgFAAIQgGAevfRxu1ax8ZolO/iaRX7b27czveUjEa1TpPWXL67ZeVYpeelaW7ezXexNLOF2UmHUKGHZ6J7dDU922rXOmqadv5Y3it0TU1UamRaR2rnZHG2dh92uvnz0ukUu8OK17Ok0JfTbuXvk4n9wrM8u1n7Tdk6qD3Lh6v5Z2hju1fafedB4frM1J6fZ7/6lfTsxa882Txq3hQACEIBgCAUAAhCAYAgFAAIQgGAIBQACEIBgCAUAAhCAYB6zcXlq58007K6tFyaH9xu52cH+6XdF17IV1ece/pkaff6ravp2fmlYrXEYa1GodVuTaX6Y+L5c/nZ0ydrf+dgkK9G6PW2Srt7O6XxphkWzsv+uJmW3vBuaX44ztdFHLmff9YmFnfy52SxtLlpdg5r1R/LhcqN/qBWKzPaze9ePp3+mf1UZ7VQt9J59HUo3hQACEIBgCAUAAhCAYAgFAAIQgGAIBQACEIBgCAUAAhCAYAgFAAIM+PxeHqlLAD8jeJNAYAgFAAIQgGAIBQACEIBgCAUAAhCAYAgFAAIQgGA5v/5v1Bqj7B8mZURAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Loading the RGB images\n",
    "dataset = YUVImageClassificationDataset(filenames=filenames, images_data=yuv_images_data)\n",
    "train_loader = DataLoader(dataset, batch_size=8, shuffle = False)\n",
    "\n",
    "for Y_channel, U_binned, V_binned, fname in train_loader:\n",
    "    \n",
    "    U_target_rec = unbin_labels(U_binned.clone().detach())\n",
    "    U_target_rec_tensor = torch.tensor(U_target_rec, dtype=torch.float32).resize(8,1,32,32)\n",
    "\n",
    "    V_target_rec = unbin_labels(V_binned.clone().detach())\n",
    "    V_target_rec_tensor = torch.tensor(V_target_rec , dtype=torch.float32).resize(8,1,32,32)\n",
    "    \n",
    "    reconstructed_img = torch.cat([Y_channel,U_target_rec_tensor,V_target_rec_tensor],dim=1)\n",
    "\n",
    "    image_yuv = reconstructed_img[0]  # Frist image of the batch (YUV)\n",
    "\n",
    "    # Need to resize first\n",
    "    rgb_output = yuv_rgb(image_yuv.resize(32, 32,3))\n",
    "    print(rgb_output[:,30,30])\n",
    "\n",
    "    print(f\"Filename: {fname[0]}\")\n",
    "\n",
    "    imshow(rgb_output) \n",
    "    \n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SIMPLIFIED NETWORK -------------\n",
    "\n",
    "\n",
    "# ----- CLASSIFICATION CNN MODEL -----\n",
    "class ClassificationCNN(nn.Module):\n",
    "    def __init__(self, num_classes=50):\n",
    "        super(ClassificationCNN, self).__init__()\n",
    "        \n",
    "        # Initial Convolutional Layers\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1), nn.ReLU()\n",
    "        )\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        \n",
    "        # Second Convolutional Block\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1), nn.ReLU()\n",
    "        )\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        \n",
    "        # Skip Connections\n",
    "        self.skip0 = nn.Conv2d(64, 128, kernel_size=3, padding = 1)\n",
    "        \n",
    "        self.conv_final = nn.Sequential(\n",
    "            nn.Conv2d(256, 128, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1), nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Output Layers for Classification\n",
    "        self.output_U = nn.Conv2d(64, num_classes, kernel_size=3, padding=1)\n",
    "        self.output_V = nn.Conv2d(64, num_classes, kernel_size=3, padding=1)\n",
    "        self.softmax = nn.Softmax(dim = 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.conv1(x)\n",
    "        x1_pooled = self.pool1(x1)\n",
    "        \n",
    "        x2 = self.conv2(x1_pooled)\n",
    "        x2_pooled = self.pool2(x2)\n",
    "        x2_pooled_padded = F.interpolate(x2_pooled, size=(32, 32), mode='bilinear', align_corners=False)\n",
    "\n",
    "        x1_conv = self.skip0(x1)\n",
    "        x2_padded = F.interpolate(x2, size=(32, 32), mode='bilinear', align_corners=False)\n",
    "        el_wise = x1_conv + x2_padded\n",
    "        \n",
    "        # Concatenation layer\n",
    "        x_concat = torch.cat([el_wise, x2_pooled_padded], dim=1) # necessitano solo stessa H e W\n",
    "\n",
    "        # Final convolutional processing\n",
    "        x_final = self.conv_final(x_concat)\n",
    "\n",
    "        U_out = self.output_U(x_final)\n",
    "        V_out = self.output_V(x_final)\n",
    "\n",
    "        return U_out, V_out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
