{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotting import imshow\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RGB training data load\n",
    "yuv_train_set = unpickle('yuv_train')\n",
    "\n",
    "# Image names \n",
    "filenames = [f.decode('utf-8') for f in yuv_train_set[b'filenames']]\n",
    "\n",
    "# Getting the images\n",
    "yuv_images_data = yuv_train_set[b'data'].copy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yuv_rgb(array):\n",
    "    new_array = array.numpy().copy()\n",
    "    new_array = cv2.cvtColor(new_array, cv2.COLOR_YUV2RGB)\n",
    "    return torch.tensor((new_array*255).astype(np.uint8)).permute(2, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YUVImageClassificationDataset(Dataset):\n",
    "    def __init__(self, filenames, images_data, bin_count=50):\n",
    "        self.filenames = filenames # image name\n",
    "        self.images_data = images_data # images data: data used to construct the images \n",
    "        self.bin_count = bin_count # number of bins used \n",
    "        self.bins = np.linspace(0, 1, bin_count)  # Bin edges\n",
    "        # we are creating bins between 0 and 1 in order to have the values of u and v\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "    \n",
    "    def bin_labels(self, values):\n",
    "        return np.digitize(values, self.bins) -1 # Map values to bin indices\n",
    "                                                   # This function takes values and assigns each value to a bin index\n",
    "                                                   # based on self.bins. It returns the index starting from 0\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Creating image from the dataset\n",
    "        img_data = self.images_data[idx]\n",
    "        img = np.array(img_data, dtype=np.uint8).reshape(3,32,32)  # (C, H, W)\n",
    "        img = np.transpose(img, (1, 2, 0))  # (H, W, C)\n",
    "\n",
    "        # Channel shapes are (H,W)\n",
    "        Y_channel = img[:, :, 0] / 255.0  # Normalize Y to [0, 1] for stability of the network \n",
    "        U_channel = img[:, :, 1] / 255.0   \n",
    "        V_channel = img[:, :, 2] / 255.0\n",
    "\n",
    "        # Binned shapes are (H,W)\n",
    "        U_binned = self.bin_labels(U_channel)  # Discretize U \n",
    "        V_binned = self.bin_labels(V_channel)  # Discretize V\n",
    "        \n",
    "        # Expanded (1,H,W)\n",
    "        Y_channel = np.expand_dims(Y_channel, axis=0)\n",
    "\n",
    "        # Tensor shapes are the same as the array shapes\n",
    "        Y_channel = torch.tensor(Y_channel, dtype=torch.float32) # transform into tensor to use pytorch \n",
    "        U_binned = torch.tensor(U_binned, dtype=torch.long)\n",
    "        V_binned = torch.tensor(V_binned, dtype=torch.long)\n",
    "       \n",
    "        return Y_channel, U_binned, V_binned, self.filenames[idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a lookup table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_count = 50\n",
    "bins = np.linspace(0, 1, bin_count)\n",
    "lookup_table = torch.zeros(50).to(device) \n",
    "for idx, value in enumerate(bins):\n",
    "    lookup_table[idx] = value\n",
    "\n",
    "def unbin_labels(bin_indices):\n",
    "    return lookup_table[bin_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the RGB images\n",
    "dataset = YUVImageClassificationDataset(filenames=filenames, images_data=yuv_images_data)\n",
    "train_loader = DataLoader(dataset, batch_size=8, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 89, 112,  51], dtype=torch.uint8)\n",
      "Filename: bos_taurus_s_000507.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_61392/3092592313.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  U_target_rec_tensor = torch.tensor(U_target_rec, dtype=torch.float32).resize(8,1,32,32)\n",
      "/tmp/ipykernel_61392/3092592313.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  V_target_rec_tensor = torch.tensor(V_target_rec , dtype=torch.float32).resize(8,1,32,32)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for Y_channel, U_binned, V_binned, fname in train_loader:\n",
    "    \n",
    "    U_target_rec = unbin_labels(U_binned.clone().detach())\n",
    "    U_target_rec_tensor = torch.tensor(U_target_rec, dtype=torch.float32).resize(8,1,32,32)\n",
    "\n",
    "    V_target_rec = unbin_labels(V_binned.clone().detach())\n",
    "    V_target_rec_tensor = torch.tensor(V_target_rec , dtype=torch.float32).resize(8,1,32,32)\n",
    "    \n",
    "    reconstructed_img = torch.cat([Y_channel.cpu(),U_target_rec_tensor.cpu(),V_target_rec_tensor.cpu()],dim=1)\n",
    "\n",
    "    image_yuv = reconstructed_img[0]  # Frist image of the batch (YUV)\n",
    "\n",
    "    # Need to resize first\n",
    "    rgb_output = yuv_rgb(image_yuv.resize(32, 32,3))\n",
    "    print(rgb_output[:,30,30])\n",
    "\n",
    "    print(f\"Filename: {fname[0]}\")\n",
    "\n",
    "    imshow(rgb_output) \n",
    "    \n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# RGB training data load\n",
    "train_set = unpickle('train')\n",
    "\n",
    "# Image names \n",
    "filenames = [f.decode('utf-8') for f in train_set[b'filenames']]\n",
    "\n",
    "# Getting the images\n",
    "images_data = train_set[b'data'].copy() \n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, filenames, images_data, transform=None):\n",
    "        self.filenames = filenames # image name\n",
    "        self.images_data = images_data # image data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Creating image from the dataset\n",
    "        img_data = self.images_data[idx]\n",
    "        img = np.array(img_data, dtype=np.uint8).reshape(3,32,32)  # (C, H, W)\n",
    "        img = np.transpose(img, (1, 2, 0))  # (H, W, C)\n",
    "\n",
    "       \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, self.filenames[idx]\n",
    "\n",
    "# Normalization and transforming to a tensor\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "dataset = CustomDataset(filenames=filenames, images_data=images_data,transform=transform)\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle = False)\n",
    "for batch in train_loader:\n",
    "    image_tensor, filename = batch  # Extract data from the batch\n",
    "    for idx, image_rgb in enumerate(image_tensor):\n",
    "        imh_rgb = image_rgb \n",
    "        imshow(imh_rgb)\n",
    "        break\n",
    "    break\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# original - created\n",
    "difference = imh_rgb -rgb_output\n",
    "print(\"R difference max: \",torch.max(difference[0,:,:]),\", min :\",torch.min(difference[0,:,:]))\n",
    "print(\"G difference max: \",torch.max(difference[1,:,:]),\", min :\",torch.min(difference[1,:,:]))\n",
    "print(\"B difference max: \",torch.max(difference[2,:,:]),\", min :\",torch.min(difference[2,:,:]))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- CLASSIFICATION CNN MODEL -----\n",
    "class ClassificationCNN(nn.Module):\n",
    "    def __init__(self, num_classes=50):\n",
    "        super(ClassificationCNN, self).__init__()\n",
    "        \n",
    "        # Initial Convolutional Layers\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1), nn.ReLU()\n",
    "        )\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        \n",
    "        # Second Convolutional Block\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1), nn.ReLU()\n",
    "        )\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        \n",
    "        # Skip Connections\n",
    "        self.skip0 = nn.Conv2d(64, 128, kernel_size=3, padding = 1)\n",
    "        \n",
    "        self.conv_final = nn.Sequential(\n",
    "            nn.Conv2d(256, 128, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1), nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Output Layers for Classification\n",
    "        self.output_U = nn.Conv2d(64, num_classes, kernel_size=3, padding=1)\n",
    "        self.output_V = nn.Conv2d(64, num_classes, kernel_size=3, padding=1)\n",
    "        self.softmax = nn.Softmax(dim = 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.conv1(x)\n",
    "        x1_pooled = self.pool1(x1)\n",
    "        \n",
    "        x2 = self.conv2(x1_pooled)\n",
    "        x2_pooled = self.pool2(x2)\n",
    "        x2_pooled_padded = F.interpolate(x2_pooled, size=(32, 32), mode='bilinear', align_corners=False)\n",
    "\n",
    "        x1_conv = self.skip0(x1)\n",
    "        x2_padded = F.interpolate(x2, size=(32, 32), mode='bilinear', align_corners=False)\n",
    "        el_wise = x1_conv + x2_padded\n",
    "        \n",
    "        # Concatenation layer\n",
    "        x_concat = torch.cat([el_wise, x2_pooled_padded], dim=1) # necessitano solo stessa H e W\n",
    "\n",
    "        # Final convolutional processing\n",
    "        x_final = self.conv_final(x_concat)\n",
    "\n",
    "        U_out = self.output_U(x_final)\n",
    "        V_out = self.output_V(x_final)\n",
    "\n",
    "        return U_out, V_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "bin_count = 50\n",
    "model = ClassificationCNN(bin_count).to(device)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Loss: 5.5435\n",
      "Epoch [2/30], Loss: 5.3249\n",
      "Epoch [3/30], Loss: 5.2804\n",
      "Epoch [4/30], Loss: 5.2573\n",
      "Epoch [5/30], Loss: 5.2427\n",
      "Epoch [6/30], Loss: 5.2319\n",
      "Epoch [7/30], Loss: 5.2233\n",
      "Epoch [8/30], Loss: 5.2162\n",
      "Epoch [9/30], Loss: 5.2108\n",
      "Epoch [10/30], Loss: 5.2062\n",
      "Epoch [11/30], Loss: 5.2020\n",
      "Epoch [12/30], Loss: 5.1983\n",
      "Epoch [13/30], Loss: 5.1947\n",
      "Epoch [14/30], Loss: 5.1920\n",
      "Epoch [15/30], Loss: 5.1893\n",
      "Epoch [16/30], Loss: 5.1866\n",
      "Epoch [17/30], Loss: 5.1842\n",
      "Epoch [18/30], Loss: 5.1823\n",
      "Epoch [19/30], Loss: 5.1805\n",
      "Epoch [20/30], Loss: 5.1791\n",
      "Epoch [21/30], Loss: 5.1772\n",
      "Epoch [22/30], Loss: 5.1755\n",
      "Epoch [23/30], Loss: 5.1743\n",
      "Epoch [24/30], Loss: 5.1728\n",
      "Epoch [25/30], Loss: 5.1715\n",
      "Epoch [26/30], Loss: 5.1702\n",
      "Epoch [27/30], Loss: 5.1690\n",
      "Epoch [28/30], Loss: 5.1675\n",
      "Epoch [29/30], Loss: 5.1671\n",
      "Epoch [30/30], Loss: 5.1653\n"
     ]
    }
   ],
   "source": [
    "total_loss = []\n",
    "# Training Loop\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for Y_channel, U_target, V_target, fname in train_loader:\n",
    "        Y_channel, U_target, V_target = Y_channel.to(device), U_target.to(device), V_target.to(device)\n",
    "        #print('U target: \\n', U_target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        U_pred, V_pred = model(Y_channel)\n",
    "        #print('U pred: \\n',U_pred)\n",
    "\n",
    "        loss_U = criterion(U_pred, U_target)\n",
    "        loss_V = criterion(V_pred, V_target)\n",
    "        loss = loss_U + loss_V\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# Save Model\n",
    "torch.save(model.state_dict(), f\"models/yuv_classification_{num_epochs}ep.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def converter(pred):\n",
    "    softmax = nn.Softmax(dim = 1).to(device)\n",
    "    probs = softmax(pred)\n",
    "    _, ind = torch.topk(probs,1,1)\n",
    "    return ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_count = 50\n",
    "bins = np.linspace(0, 1, bin_count)\n",
    "lookup_table = torch.zeros(50).to(device) \n",
    "for idx, value in enumerate(bins):\n",
    "    lookup_table[idx] = value\n",
    "\n",
    "def unbin_labels(bin_indices):\n",
    "    return lookup_table[bin_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"models/yuv_classification_30ep.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 50, 32, 32])\n",
      "torch.Size([8, 1, 32, 32])\n",
      "Filename: bos_taurus_s_000507.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_61392/3702553855.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pred_U_rec_tensor = torch.tensor(pred_U_rec, dtype=torch.float32).resize(8,1,32,32)\n",
      "/tmp/ipykernel_61392/3702553855.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pred_V_rec_tensor = torch.tensor(pred_V_rec, dtype=torch.float32).resize(8,1,32,32)\n",
      "/tmp/ipykernel_61392/3702553855.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  U_target_rec_tensor = torch.tensor(U_target_rec, dtype=torch.float32).resize(8,1,32,32)\n",
      "/tmp/ipykernel_61392/3702553855.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  V_target_rec_tensor = torch.tensor(V_target_rec , dtype=torch.float32).resize(8,1,32,32)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAEc5JREFUeJzt3F1s3Xd5B/CfHdtpYrcJeaOchBIY7bQCK+rOxsugFInBykAWFdwwpN2wiWgau94kX0Ubu5w0mLnZmJBAkybQTAFpLWKFUUapoeOtrfJGkianTpw31y+xj+1zdjHpEVqR/HtK/41NPp/rLw//+vj02/+FvwP9fr9fAKCUMnijHwCAzUMpABCUAgBBKQAQlAIAQSkAEJQCAEEpABCGaoP90mvyOWDL6a+vVGdnzpxM3X78+z+szt73B+9L3d6zd38qv1Wtl/q/y11az/37bX7hSnX25ImnUrf37Burzp45ezx1+4/e+dENM94UAAhKAYCgFAAISgGAoBQACEoBgKAUAAhKAYCgFAAISgGAoBQACNXbR1n9fv3uCGwGvfXVVH5gtX7/Zv5ibvvo0Ye+XJ1dmF9O3f7Yn368PtzLfY97vfoNof62gdTtrNX1+mxn5kzq9pWrnerszLmnU7dPnbhYnZ2bu5q6XWwfAZChFAAISgGAoBQACEoBgKAUAAhKAYCgFAAISgGAoBQACI3NXGQMDDT75+7cnHolN9EwOJDL99fn68PLl1O3d/ZWqrOXZ55L3b7w3Ex1dttg7r8bb9u1qzo7MjiSur2e/G/Yfn+tOjs0nDpduuuL1dm9B16Rur06O1udnTlZ/1nW8qYAQFAKAASlAEBQCgAEpQBAUAoABKUAQFAKAASlAEBQCgAEpQBA2BTbRzeLzBZPv9dN3V69cimVX56r3+3pbx9N3b7t4MH6cHb3ql//MxzsraZOzz93NpX/+U8fr88+/Uzq9rbB7dXZ5587l7r96Ne/XJ19xcE7Urff9vZ3VGcHhnLbR5evXU3llxfqN6GWV3Lfn/7aQnX2wpWfp25fm6t/lt566nQVbwoABKUAQFAKAASlAEBQCgAEpQBAUAoABKUAQFAKAASlAEAwc/Ey6q/3qrOzJ3OzCLM/eCyVX7x8rTp7oZv7b4c733l/dfaue34ndXtwpP5X9ic//Wnq9pOPfjOVX+jUz0vMXZxJ3R7aVj8Bcf3y+dTtb37tdHX27vvfm7r9tvveXZ1d7q6kbl+dPZ3Kn3zi69XZC51Tqdt773h1dXapt5S6vbo4XJ3dvu1A6nYNbwoABKUAQFAKAASlAEBQCgAEpQBAUAoABKUAQFAKAASlAEBQCgAE20cvo/5K/dbL5WdO5m5fm0vl9wytV2cH1q6nbp/6r4ers0P9gdTtHa07qrOf/9JDqds/e+JHqfzr9uyozu4ZWEvdHh2u37/pJXaSSinl58fqt5IeO/7l1O3bD72xOvvO3/2t1O2LT/93Kv/jh/+9Ort87Urq9uLhu6uzo3e3U7d37txbnR177Z7U7RreFAAISgGAoBQACEoBgKAUAAhKAYCgFAAISgGAoBQACEoBgNDYzEVmvSA3dLB1DQ7XzxGMvbKVun3p/OlUfuXiuerszpFe6vbIcn32me99J3V7ac/h6uzD//FY7vb8Qip/6+Cr6rOvyE1RLK7Uz2I8c/a51O3nFuq/cc9ezs0/fPFzn6u//eSB1O3r536Qyo+uLVZnt++onywppZTuUv30y+Fb96VuD7zyzursykD9HEotbwoABKUAQFAKAASlAEBQCgAEpQBAUAoABKUAQFAKAASlAEBQCgCExraPBvqZcFNPUUrpZR6klDLY4MMMb6uO3v6me1KnV+evpvKnzj5TnR25Opt7lpH6HZnjx55K3Z4fW6rODq3mNpvmr1xO5Z/fO1qd3fGa+j2bUkqZu3atOvvjM7nto9mVW6qzt+66NXX79IkfVWe3X6nfDyqllDv35/ajRobrv/vXVnL/nrjtQP3v+Pnzz6Zu796xpzo7vDe3q1TDmwIAQSkAEJQCAEEpABCUAgBBKQAQlAIAQSkAEJQCAEEpABAam7nIzEv06tcf0gb6uT9fH8gsIyQnMQbW659lODEVUUopB9/y+6l8Ga5/9pknH0udPvSqQ9XZS5dyUxQ/evzJ6uyOoeXU7b1juRmF++97W3X2Lfe8IXX7Hz7z6ers/NJq6vbI9p314bWF1O3lxfoZku131M85lFJKr9dN5S9cnKvODu++PXV7YHR/dfbHPzuZuj33g6ers63feF3q9kfe/d4NM94UAAhKAYCgFAAISgGAoBQACEoBgKAUAAhKAYCgFAAISgGAoBQACNXbR71ebqMmswrUT+4TLa+sVGdHhodTtwcH6ntyW+qfsqS2khIzSaWUUk5duZLKX91+S3V25c7cbs/d7XdUZ7tnLqVu/9vXv1GdXb2+mLr9oQfelco/+IH3VWePnTiVun1hsf771u3nJsxGylp1dngoN0w2tmN7fXZX/X5QKaXMrdbvKpVSytiB+j2j/o5dqdvnZus3oXrXr6dud+eer87+51e+krpd/u7vN4x4UwAgKAUAglIAICgFAIJSACAoBQCCUgAgKAUAglIAICgFAIJSACBUj6asrHZTh28Zqd9AeX5xPnX7O088Xp29bezW1O173/Cm6uytO0dTt9fX16uz52Y7qduPfueRVP7U2TPV2e713Gc/0nptdXZtfjl1+8Lp+udemM9tH73uNXek8kOl/vO8NjeXut3t1W8OrfXqn6OUUlbXf1KdfdM9b03dnvrSd1P5zWL8Q+9J5VuHdteHd+Se5TfvPVifLfXZWt4UAAhKAYCgFAAISgGAoBQACEoBgKAUAAhKAYCgFAAISgGAMNDv9/s1wevruckAXujopz5Zne3MHM8d7+UmN8pg/efZOZebaGi32/Xhtf2p29PT09XZVquVul0Gl5L5+p/h2Nrm+f4cu1Q/oTK2lPt8vvC1b2QfpzFHjx6tD2c/+6GLufwm8bd//a8bZrwpABCUAgBBKQAQlAIAQSkAEJQCAEEpABCUAgBBKQAQlAIAQSkAEIZu9AM07ciRI6l8ai8nu5fSq9+Rae3Lbc6MldOpfKdTv8Vz1547U7dT+0QH7kndznw+Y2vJzycr8Z9UCyO5baqR1fp8duOpdeDN1dnUjlUpZfxD76kPZ/e6ku46sLs6uzCSPJ7YvWod2pW7nfm5JJ6j+uRLfhGALUspABCUAgBBKQAQlAIAQSkAEJQCAEEpABCUAgBBKQAQBvr9fr8meH39pf9z6pvN0U99sjo7tpSbuTh25UQq/9bR+mmEh5/+Vur2G/fW/1n/9+bnUrczf9Z/1+43p053Lv5PKt+6PTf/kZKYOuh0zuduZ+ZZejsbu/2Fh76bOv3HH3x78lnqf1eycx4Zx2bWUvnMs2QnNB78w41nf7wpABCUAgBBKQAQlAIAQSkAEJQCAEEpABCUAgBBKQAQlAIAQSkAEIZu9APcTI58fKI6e77TSd2+7YffTuU7jzxVnf29g69P3V5anq3OTj30xdTt8Qf+sjp77OK11O1SDqfSnXP191uHdqduL4zUZ/e9+mDq9qVn67eSPvbJjbdyftFn//HzqXzGwi25/bWxbn12eno6dbvVqt8Oa92R28jK/F41wZsCAEEpABCUAgBBKQAQlAIAQSkAEJQCAEEpABCUAgBBKQAQBvr9fr8meH099yfmvNCl2fr5h8nPfjZ1uzNzIpW//7fb1dlT36qfxMg6uyuXn5ycrM6Oj4+nbneS0yKZqYO0wQa/b73R5m5nJP8Zx3r1359SSllIzpZkpD77waXc7dvrZ2Xa7frvcSmlPPiBD2+Y8aYAQFAKAASlAEBQCgAEpQBAUAoABKUAQFAKAASlAEBQCgAEpQBAGLrRD8BLpLczFZ/+fv2e0bHF5jaBMltGTWtyy2hkPfk/qFok+z+Xns19PgtD9dtHTf5M2ve+q7HbpeS3rDKeP3u8sdudzvnq7PT0dOq27SMAUpQCAEEpABCUAgBBKQAQlAIAQSkAEJQCAEEpABCUAgBBKQAQbB+9jPbt31+dzW6atNvtVD5zP7t/k9kzGh8fT93eqlqHdqfync5idTazZbSZPPPd3O941sJIc7fHhmars91+/fe+lFJK78Z+nt4UAAhKAYCgFAAISgGAoBQACEoBgKAUAAhKAYCgFAAISgGAYObi10R2FiOj0+mk8kePHm3oSTaXsW59ttM5n7rdHchMHcylbmc/z4zMJEqTMxSNW0tMV2xL3h6snzhpYhLDmwIAQSkAEJQCAEEpABCUAgBBKQAQlAIAQSkAEJQCAEEpABCUAgBhS24fjY+PN3Z7amqqsds3i820w5Rx17763Z5SSukO12/UjKznnuX0xc2xT5SV+Xyyz/H82eOpfJP/nN3snlFC51z9ltXctTMv+f+/NwUAglIAICgFAIJSACAoBQCCUgAgKAUAglIAICgFAIJSACBsyZmLzWKkXz9zUEop3YHRhp5kc2ly6iAj+/m03/qaVH76+0+l8hlN/lwyMyTtdrux5yiDuc/ntsO5n8lCIju2tpS6XcrOZL5e69Cuxm7X8KYAQFAKAASlAEBQCgAEpQBAUAoABKUAQFAKAASlAEBQCgAEpQBAGOj3+/2a4PpgN3X4gfe//0U90FaS3aeZnJyszo6Pj2cfpzlDs6l4+80PNPQgpUxMTFRnjxw50thzlFJKGczu5WwSvfrdnsyOVSmlTE1NZZ9mUzjy53/S3PHEz7uUUsrI6UYeo5RS/uXT394w400BgKAUAAhKAYCgFAAISgGAoBQACEoBgKAUAAhKAYCgFAAIQ7XB7upq6vBU2Zp/7t6kI3/x4epsdkIj/afxvdHq6ORnHkmdHv/gR6uz2VmEzHRF9meYnXRIGVzM5df2V0dbh3anTnfOXcs9yxZ19OjR+nCTUxTdw7nbie9m+veq5uRLfhGALUspABCUAgBBKQAQlAIAQSkAEJQCAEEpABCUAgBBKQAQlAIAoXr76GaR2dbJq9+zSVs7kMtnNlMyWywlv2fUlPSW0eBSMw9SSvpnmNlt6nROJG+/vjo7OTmZur2ZNLpllfi+HX5t7nvfXd1dne3MHE/druFNAYCgFAAISgGAoBQACEoBgKAUAAhKAYCgFAAISgGAoBQACGYufgXZCYBmJzQalJnEKCU96ZDR5OxCo59PckKjM5ObruCFNstEx9G/+atUfnTnzups6/Y7s4+zIW8KAASlAEBQCgAEpQBAUAoABKUAQFAKAASlAEBQCgAEpQBAUAoABNtH/0+TeymZ29kdnva996Xy0z/8dn24wS2jzWTL7ioldTqdG/0IN5de/ZZRKaUsLtRnW63XJx9mY94UAAhKAYCgFAAISgGAoBQACEoBgKAUAAhKAYCgFAAISgGAoBQACLaPXkaXZmers61WK3U7u2eT3UriV9Pk3lD2dyWb54Wa3LK60Z+PNwUAglIAICgFAIJSACAoBQCCUgAgKAUAglIAICgFAIJSACCYudikpqenU/l2u93Qk5QyPj6eyk9NTTX0JPwy2QmNzO/WxMRE9nG2pOxsRZOzJZnb6UmMP9s44k0BgKAUAAhKAYCgFAAISgGAoBQACEoBgKAUAAhKAYCgFAAISgGAYPvoZbRv//7Gbme3kj7xiU809CT8MiPruXx3WzPPUUqzuz1b1eTkZCqf3QNrShOfpTcFAIJSACAoBQCCUgAgKAUAglIAICgFAIJSACAoBQCCUgAgDPT7/X5N8Pr6YtPPwi9o+s/oW61WdTb7p/RTU1PZx/m1l525eODB5j7/Jj/7drtdnZ2YmEjd3qo+8oHcZ9nkxMnDX/3GhhlvCgAEpQBAUAoABKUAQFAKAASlAEBQCgAEpQBAUAoABKUAQFAKAIShG/0A/HKZfZpS8hs109PTjT1Lk7tNdpVeKPvZZz7P7M8789lnf08202efefaprzb3M2yCNwUAglIAICgFAIJSACAoBQCCUgAgKAUAglIAICgFAIJSACAM9Pv9fk1wfbCbOtxdXX1RD8SLc+TIkVQ+M3PxxBNPpG7f6D/Tf7GyMwqZn/nk5GRjt7M+90//XJ1d7q6kbmc++3a7nbo9MTGRyjf5+TT1HKU0+yw7to1umPGmAEBQCgAEpQBAUAoABKUAQFAKAASlAEBQCgAEpQBAUAoABKUAQKjePgLg1583BQCCUgAgKAUAglIAICgFAIJSACAoBQCCUgAgKAUAwv8C0ALOZGCgaQUAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for Y_channel, U_target, V_target, fname in train_loader:\n",
    "    Y_channel, U_target, V_target = Y_channel.to(device), U_target.to(device), V_target.to(device)\n",
    "    \n",
    "    U_pred, V_pred = model(Y_channel)\n",
    "\n",
    "    print(U_pred.size())\n",
    "    conv_pred_U = converter(U_pred.cpu())\n",
    "    conv_pred_V = converter(V_pred.cpu())\n",
    "\n",
    "    pred_U_rec = unbin_labels(conv_pred_U)\n",
    "    print(pred_U_rec.size())\n",
    "    pred_U_rec_tensor = torch.tensor(pred_U_rec, dtype=torch.float32).resize(8,1,32,32)\n",
    "    pred_V_rec = unbin_labels(conv_pred_V )\n",
    "    pred_V_rec_tensor = torch.tensor(pred_V_rec, dtype=torch.float32).resize(8,1,32,32)\n",
    "\n",
    "\n",
    "    U_target_rec = unbin_labels(U_target)\n",
    "    U_target_rec_tensor = torch.tensor(U_target_rec, dtype=torch.float32).resize(8,1,32,32)\n",
    "\n",
    "    V_target_rec = unbin_labels(V_target)\n",
    "    V_target_rec_tensor = torch.tensor(V_target_rec , dtype=torch.float32).resize(8,1,32,32)\n",
    "\n",
    "    reconstructed_pred = torch.cat([Y_channel,pred_U_rec_tensor,pred_V_rec_tensor],dim=1)\n",
    "    \n",
    "    reconstructed_img = torch.cat([Y_channel,U_target_rec_tensor,V_target_rec_tensor],dim=1)\n",
    "\n",
    "    image_yuv = reconstructed_img[0].cpu()  # Frist image of the batch (YUV)\n",
    "    image_yuv_pred = reconstructed_pred[0].cpu()\n",
    "\n",
    "    # Need to resize first\n",
    "    rgb_output = yuv_rgb(image_yuv.resize(32, 32,3))\n",
    "    rgb_output_pred = yuv_rgb(image_yuv_pred.resize(32, 32,3))\n",
    "\n",
    "    print(f\"Filename: {fname[0]}\")\n",
    "\n",
    "    #imshow(rgb_output) \n",
    "    imshow(rgb_output_pred) \n",
    "    \n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
