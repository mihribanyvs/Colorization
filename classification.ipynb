{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotting import imshow\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RGB training data load\n",
    "yuv_train_set = unpickle('yuv_train')\n",
    "\n",
    "# Image names \n",
    "filenames = [f.decode('utf-8') for f in yuv_train_set[b'filenames']]\n",
    "\n",
    "# Getting the images\n",
    "yuv_images_data = yuv_train_set[b'data'].copy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yuv_rgb(array):\n",
    "    new_array = array.numpy().copy()\n",
    "    new_array = cv2.cvtColor(new_array, cv2.COLOR_YUV2RGB)\n",
    "    return torch.tensor((new_array*255).astype(np.uint8)).permute(2, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YUVImageClassificationDataset(Dataset):\n",
    "    def __init__(self, filenames, images_data, bin_count=50):\n",
    "        self.filenames = filenames # image name\n",
    "        self.images_data = images_data # images data: data used to construct the images \n",
    "        self.bin_count = bin_count # number of bins used \n",
    "        self.bins = np.linspace(0, 1, bin_count)  # Bin edges\n",
    "        # we are creating bins between 0 and 1 in order to have the values of u and v\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "    \n",
    "    def bin_labels(self, values):\n",
    "        return np.digitize(values, self.bins) -1 # Map values to bin indices\n",
    "                                                   # This function takes values and assigns each value to a bin index\n",
    "                                                   # based on self.bins. It returns the index starting from 0\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Creating image from the dataset\n",
    "        img_data = self.images_data[idx]\n",
    "        img = np.array(img_data, dtype=np.uint8).reshape(3,32,32)  # (C, H, W)\n",
    "        img = np.transpose(img, (1, 2, 0))  # (H, W, C)\n",
    "\n",
    "        # Channel shapes are (H,W)\n",
    "        Y_channel = img[:, :, 0] / 255.0  # Normalize Y to [0, 1] for stability of the network \n",
    "        U_channel = img[:, :, 1] / 255.0   \n",
    "        V_channel = img[:, :, 2] / 255.0\n",
    "\n",
    "        # Binned shapes are (H,W)\n",
    "        U_binned = self.bin_labels(U_channel)  # Discretize U \n",
    "        V_binned = self.bin_labels(V_channel)  # Discretize V\n",
    "        \n",
    "        # Expanded (1,H,W)\n",
    "        Y_channel = np.expand_dims(Y_channel, axis=0)\n",
    "\n",
    "        # Tensor shapes are the same as the array shapes\n",
    "        Y_channel = torch.tensor(Y_channel, dtype=torch.float32) # transform into tensor to use pytorch \n",
    "        U_binned = torch.tensor(U_binned, dtype=torch.long)\n",
    "        V_binned = torch.tensor(V_binned, dtype=torch.long)\n",
    "       \n",
    "        return Y_channel, U_binned, V_binned, self.filenames[idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a lookup table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_count = 50\n",
    "bins = np.linspace(0, 1, bin_count)\n",
    "lookup_table = torch.zeros(50) \n",
    "for idx, value in enumerate(bins):\n",
    "    lookup_table[idx] = value\n",
    "\n",
    "def unbin_labels(bin_indices):\n",
    "    return lookup_table[bin_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the RGB images\n",
    "dataset = YUVImageClassificationDataset(filenames=filenames, images_data=yuv_images_data)\n",
    "train_loader = DataLoader(dataset, batch_size=8, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for Y_channel, U_binned, V_binned, fname in train_loader:\n",
    "    \n",
    "    U_target_rec = unbin_labels(U_binned.clone().detach())\n",
    "    U_target_rec_tensor = torch.tensor(U_target_rec, dtype=torch.float32).resize(8,1,32,32)\n",
    "\n",
    "    V_target_rec = unbin_labels(V_binned.clone().detach())\n",
    "    V_target_rec_tensor = torch.tensor(V_target_rec , dtype=torch.float32).resize(8,1,32,32)\n",
    "    \n",
    "    reconstructed_img = torch.cat([Y_channel,U_target_rec_tensor,V_target_rec_tensor],dim=1)\n",
    "\n",
    "    image_yuv = reconstructed_img[0]  # Frist image of the batch (YUV)\n",
    "\n",
    "    # Need to resize first\n",
    "    rgb_output = yuv_rgb(image_yuv.resize(32, 32,3))\n",
    "    print(rgb_output[:,30,30])\n",
    "\n",
    "    print(f\"Filename: {fname[0]}\")\n",
    "\n",
    "    imshow(rgb_output) \n",
    "    \n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RGB training data load\n",
    "train_set = unpickle('train')\n",
    "\n",
    "# Image names \n",
    "filenames = [f.decode('utf-8') for f in train_set[b'filenames']]\n",
    "\n",
    "# Getting the images\n",
    "images_data = train_set[b'data'].copy() \n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, filenames, images_data, transform=None):\n",
    "        self.filenames = filenames # image name\n",
    "        self.images_data = images_data # image data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Creating image from the dataset\n",
    "        img_data = self.images_data[idx]\n",
    "        img = np.array(img_data, dtype=np.uint8).reshape(3,32,32)  # (C, H, W)\n",
    "        img = np.transpose(img, (1, 2, 0))  # (H, W, C)\n",
    "\n",
    "       \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, self.filenames[idx]\n",
    "\n",
    "# Normalization and transforming to a tensor\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(filenames=filenames, images_data=images_data,transform=transform)\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle = False)\n",
    "for batch in train_loader:\n",
    "    image_tensor, filename = batch  # Extract data from the batch\n",
    "    for idx, image_rgb in enumerate(image_tensor):\n",
    "        imh_rgb = image_rgb \n",
    "        imshow(imh_rgb)\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original - created\n",
    "difference = imh_rgb -rgb_output\n",
    "print(\"R difference max: \",torch.max(difference[0,:,:]),\", min :\",torch.min(difference[0,:,:]))\n",
    "print(\"G difference max: \",torch.max(difference[1,:,:]),\", min :\",torch.min(difference[1,:,:]))\n",
    "print(\"B difference max: \",torch.max(difference[2,:,:]),\", min :\",torch.min(difference[2,:,:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- CLASSIFICATION CNN MODEL -----\n",
    "class ClassificationCNN(nn.Module):\n",
    "    def __init__(self, num_classes=50):\n",
    "        super(ClassificationCNN, self).__init__()\n",
    "        \n",
    "        # Initial Convolutional Layers\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1), nn.ReLU()\n",
    "        )\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        \n",
    "        # Second Convolutional Block\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1), nn.ReLU()\n",
    "        )\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        \n",
    "        # Skip Connections\n",
    "        self.skip0 = nn.Conv2d(64, 128, kernel_size=3, padding = 1)\n",
    "        \n",
    "        self.conv_final = nn.Sequential(\n",
    "            nn.Conv2d(256, 128, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1), nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Output Layers for Classification\n",
    "        self.output_U = nn.Conv2d(64, num_classes, kernel_size=3, padding=1)\n",
    "        self.output_V = nn.Conv2d(64, num_classes, kernel_size=3, padding=1)\n",
    "        self.softmax = nn.Softmax(dim = 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.conv1(x)\n",
    "        x1_pooled = self.pool1(x1)\n",
    "        \n",
    "        x2 = self.conv2(x1_pooled)\n",
    "        x2_pooled = self.pool2(x2)\n",
    "        x2_pooled_padded = F.interpolate(x2_pooled, size=(32, 32), mode='bilinear', align_corners=False)\n",
    "\n",
    "        x1_conv = self.skip0(x1)\n",
    "        x2_padded = F.interpolate(x2, size=(32, 32), mode='bilinear', align_corners=False)\n",
    "        el_wise = x1_conv + x2_padded\n",
    "        \n",
    "        # Concatenation layer\n",
    "        x_concat = torch.cat([el_wise, x2_pooled_padded], dim=1) # necessitano solo stessa H e W\n",
    "\n",
    "        # Final convolutional processing\n",
    "        x_final = self.conv_final(x_concat)\n",
    "\n",
    "        U_out = self.output_U(x_final)\n",
    "        V_out = self.output_V(x_final)\n",
    "\n",
    "        return U_out, V_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ClassificationCNN(bin_count).to(device)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss = []\n",
    "# Training Loop\n",
    "num_epochs = 2\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for Y_channel, U_target, V_target, fname in train_loader:\n",
    "        Y_channel, U_target, V_target = Y_channel.to(device), U_target.to(device), V_target.to(device)\n",
    "        #print('U target: \\n', U_target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        U_pred, V_pred = model(Y_channel)\n",
    "        #print('U pred: \\n',U_pred)\n",
    "\n",
    "        loss_U = criterion(U_pred, U_target)\n",
    "        loss_V = criterion(V_pred, V_target)\n",
    "        loss = loss_U + loss_V\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# Save Model\n",
    "torch.save(model.state_dict(), f\"models/yuv_classification_{num_epochs}ep.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
