{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotting import imshow\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RGB training data load\n",
    "yuv_train_set = unpickle('yuv_train')\n",
    "\n",
    "# Image names \n",
    "filenames = [f.decode('utf-8') for f in yuv_train_set[b'filenames']]\n",
    "\n",
    "# Getting the images\n",
    "yuv_images_data = yuv_train_set[b'data'].copy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input tensor(H,W,C) -> output tensor(C,H,W)\n",
    "def yuv_rgb(array):\n",
    "    new_array = array.numpy().astype(np.float32).copy()\n",
    "    new_array = cv2.cvtColor(new_array, cv2.COLOR_YUV2RGB)\n",
    "    return torch.tensor((new_array*255).astype(np.uint8)).permute(2, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YUVImageClassificationDataset(Dataset):\n",
    "    def __init__(self, filenames, images_data, bin_count=50):\n",
    "        self.filenames = filenames # image name\n",
    "        self.images_data = images_data # images data: data used to construct the images \n",
    "        self.bin_count = bin_count # number of bins used \n",
    "        self.bins = np.linspace(0, 1, bin_count)  # Bin edges\n",
    "        # we are creating bins between 0 and 1 in order to have the values of u and v\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "    \n",
    "    def bin_labels(self, values):\n",
    "        return np.digitize(values, self.bins) -1 # Map values to bin indices\n",
    "                                                   # This function takes values and assigns each value to a bin index\n",
    "                                                   # based on self.bins. It returns the index starting from 0\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Creating image from the dataset\n",
    "        img_data = self.images_data[idx]\n",
    "        img = np.array(img_data, dtype=np.uint8)\n",
    "\n",
    "        Y_channel = img[0:1024].reshape(32, 32, 1) / 255\n",
    "        U_channel = img[1024:2048].reshape(32, 32, 1) / 255\n",
    "        V_channel = img[2048:].reshape(32, 32, 1) / 255\n",
    "        #return y, u, v, self.filenames[idx]\n",
    "\n",
    "    \n",
    "        #return Y_channel, U_channel, V_channel, self.filenames[idx]\n",
    "\n",
    "        # Binned shapes are (H,W)\n",
    "        U_binned = self.bin_labels(U_channel)  # Discretize U \n",
    "        V_binned = self.bin_labels(V_channel)  # Discretize V\n",
    "\n",
    "\n",
    "        return Y_channel, U_binned, V_binned, self.filenames[idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a lookup table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_count = 50\n",
    "bins = np.linspace(0, 1, bin_count)\n",
    "lookup_table = torch.zeros(50).to(device) \n",
    "for idx, value in enumerate(bins):\n",
    "    lookup_table[idx] = value\n",
    "\n",
    "def unbin_labels(bin_indices):\n",
    "    return lookup_table[bin_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the RGB images\n",
    "yuv_dataset = YUVImageClassificationDataset(filenames=filenames, images_data=yuv_images_data)\n",
    "yuv_train_loader = DataLoader(yuv_dataset, batch_size=8, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for Y_channel, U_binned, V_binned, fname in yuv_train_loader:\n",
    "\n",
    "    \n",
    "    # shape (8,32,32,1) \n",
    "    U_target_rec = unbin_labels(U_binned.clone().detach())\n",
    "    V_target_rec = unbin_labels(V_binned.clone().detach())\n",
    "    \n",
    "    #shape (8,3,32,32)\n",
    "    img_batch = torch.cat([Y_channel.cpu(),U_target_rec.cpu(),V_target_rec.cpu()],dim=3).permute(0,3,1,2)\n",
    "    \n",
    "    imggg = img_batch[7]\n",
    "    imshow(imggg)\n",
    "    rgb_output = yuv_rgb(imggg.permute(1,2,0))\n",
    "\n",
    "    imshow(rgb_output) \n",
    "    \n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- CLASSIFICATION CNN MODEL -----\n",
    "class ClassificationCNN(nn.Module):\n",
    "    def __init__(self, num_classes=50):\n",
    "        super(ClassificationCNN, self).__init__()\n",
    "        \n",
    "        # Initial Convolutional Layers\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1), nn.ReLU()\n",
    "        )\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        \n",
    "        # Second Convolutional Block\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1), nn.ReLU()\n",
    "        )\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        \n",
    "        # Skip Connections\n",
    "        self.skip0 = nn.Conv2d(64, 128, kernel_size=3, padding = 1)\n",
    "        \n",
    "        self.conv_final = nn.Sequential(\n",
    "            nn.Conv2d(256, 128, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1), nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Output Layers for Classification\n",
    "        self.output_U = nn.Conv2d(64, num_classes, kernel_size=3, padding=1)\n",
    "        self.output_V = nn.Conv2d(64, num_classes, kernel_size=3, padding=1)\n",
    "        self.softmax = nn.Softmax(dim = 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.conv1(x)\n",
    "        x1_pooled = self.pool1(x1)\n",
    "        \n",
    "        x2 = self.conv2(x1_pooled)\n",
    "        x2_pooled = self.pool2(x2)\n",
    "        x2_pooled_padded = F.interpolate(x2_pooled, size=(32, 32), mode='bilinear', align_corners=False)\n",
    "\n",
    "        x1_conv = self.skip0(x1)\n",
    "        x2_padded = F.interpolate(x2, size=(32, 32), mode='bilinear', align_corners=False)\n",
    "        el_wise = x1_conv + x2_padded\n",
    "        \n",
    "        # Concatenation layer\n",
    "        x_concat = torch.cat([el_wise, x2_pooled_padded], dim=1) # necessitano solo stessa H e W\n",
    "\n",
    "        # Final convolutional processing\n",
    "        x_final = self.conv_final(x_concat)\n",
    "\n",
    "        U_out = self.output_U(x_final)\n",
    "        V_out = self.output_V(x_final)\n",
    "\n",
    "        return U_out, V_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bin_count = 50\n",
    "model = ClassificationCNN(bin_count).to(device)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss = []\n",
    "# Training Loop\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for Y_channel, U_target, V_target, fname in yuv_train_loader:\n",
    "        Y_channel, U_target, V_target = Y_channel.to(device), U_target.to(device), V_target.to(device)\n",
    "        #print('U target: \\n', U_target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        U_pred, V_pred = model(Y_channel)\n",
    "        #print('U pred: \\n',U_pred)\n",
    "\n",
    "        loss_U = criterion(U_pred, U_target)\n",
    "        loss_V = criterion(V_pred, V_target)\n",
    "        loss = loss_U + loss_V\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(yuv_train_loader):.4f}\")\n",
    "\n",
    "# Save Model\n",
    "torch.save(model.state_dict(), f\"models/yuv_classification_{num_epochs}ep.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def converter(pred):\n",
    "    softmax = nn.Softmax(dim = 1).to(device)\n",
    "    probs = softmax(pred)\n",
    "    _, ind = torch.topk(probs,1,1)\n",
    "    return ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_count = 50\n",
    "bins = np.linspace(0, 1, bin_count)\n",
    "lookup_table = torch.zeros(50).to(device) \n",
    "for idx, value in enumerate(bins):\n",
    "    lookup_table[idx] = value\n",
    "\n",
    "def unbin_labels(bin_indices):\n",
    "    return lookup_table[bin_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"models/yuv_classification_30ep.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for Y_channel, U_target, V_target, fname in yuv_train_loader:\n",
    "    Y_channel, U_target, V_target = Y_channel.to(device), U_target.to(device), V_target.to(device)\n",
    "    \n",
    "    U_pred, V_pred = model(Y_channel)\n",
    "    #not sure\n",
    "    conv_pred_U = converter(U_pred.cpu())\n",
    "    conv_pred_V = converter(V_pred.cpu())\n",
    "\n",
    "    pred_U_rec = unbin_labels(conv_pred_U.clone().detach())\n",
    "    #print(pred_U_rec.size())\n",
    "    #pred_U_rec_tensor = torch.tensor(pred_U_rec, dtype=torch.float32).resize(8,1,32,32)\n",
    "    pred_V_rec = unbin_labels(conv_pred_V.clone().detach())\n",
    "    #pred_V_rec_tensor = torch.tensor(pred_V_rec, dtype=torch.float32).resize(8,1,32,32)\n",
    "    reconstructed_pred = torch.cat([Y_channel.cpu(),pred_U_rec.cpu(),pred_V_rec.cpu()],dim=3).permute(0,3,1,2)\n",
    "\n",
    "    U_target_rec = unbin_labels(U_target)\n",
    "    #U_target_rec_tensor = torch.tensor(U_target_rec, dtype=torch.float32).resize(8,1,32,32)\n",
    "\n",
    "    V_target_rec = unbin_labels(V_target)\n",
    "    reconstructed_img = torch.cat([Y_channel.cpu(),U_target_rec.cpu(),V_target_rec.cpu()],dim=3).permute(0,3,1,2)\n",
    "    #V_target_rec_tensor = torch.tensor(V_target_rec , dtype=torch.float32).resize(8,1,32,32)\n",
    "\n",
    "    #reconstructed_pred = torch.cat([Y_channel,pred_U_rec_tensor,pred_V_rec_tensor],dim=1)\n",
    "    \n",
    "    #reconstructed_img = torch.cat([Y_channel,U_target_rec_tensor,V_target_rec_tensor],dim=1)\n",
    "\n",
    "    image_yuv = reconstructed_img[0]  # Frist image of the batch (YUV)\n",
    "    image_yuv_pred = reconstructed_pred[0]\n",
    "\n",
    "    # Need to resize first\n",
    "    rgb_output = yuv_rgb(image_yuv.permute(1,2,0))\n",
    "    rgb_output_pred = yuv_rgb(image_yuv_pred.permute(1,2,0))\n",
    "\n",
    "    print(f\"Filename: {fname[0]}\")\n",
    "\n",
    "    #imshow(rgb_output) \n",
    "    imshow(rgb_output_pred) \n",
    "    \n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
